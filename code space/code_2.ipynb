{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae916ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===========================\n",
    "# Diffusion schedule\n",
    "# ===========================\n",
    "T = 1000\n",
    "beta_start = 1e-4\n",
    "beta_end   = 2e-2\n",
    "betas = torch.linspace(beta_start, beta_end, T, device=device)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "sigmas = torch.sqrt(1.0 - alpha_bar)\n",
    "\n",
    "# ===========================\n",
    "# Data: mixture Gaussian (3D)\n",
    "# ===========================\n",
    "mu1_single = torch.tensor([+2.0, 0.0, 0.0], device=device)  # cluster 1\n",
    "mu2_single = torch.tensor([-2.0, 0.0, 0.0], device=device)  # cluster 2\n",
    "\n",
    "\n",
    "N_POINTS = 64\n",
    "\n",
    "def sample_x0_cloud(batch_size: int, n_points: int = N_POINTS):\n",
    "    # one label for entire cloud\n",
    "    mix = torch.bernoulli(0.5 * torch.ones(batch_size, device=device))  # (B,)\n",
    "    \n",
    "    # assign whole cloud to cluster 1 or 2\n",
    "    mu_cloud = torch.where(\n",
    "        mix[:, None, None] == 1,\n",
    "        mu1_single, \n",
    "        mu2_single\n",
    "    )  # (B,1,3)\n",
    "\n",
    "    eps = torch.randn(batch_size, n_points, 3, device=device)\n",
    "    x0 = mu_cloud + eps    # (B,N,3)\n",
    "\n",
    "    return x0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c0be11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Fourier time embedding\n",
    "# ===========================\n",
    "class FourierTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim=64, max_freq=1000.0):\n",
    "        super().__init__()\n",
    "        self.freqs = torch.exp(\n",
    "            torch.linspace(0, math.log(max_freq), dim//2)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_norm = t.float() / float(T)\n",
    "        args = t_norm.unsqueeze(-1) * self.freqs.to(t.device)\n",
    "        return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "# ===========================\n",
    "# EGNN Layer\n",
    "# ===========================\n",
    "class EGNNLayer(nn.Module):\n",
    "    def __init__(self, time_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2 + 1 + time_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h, t_emb):\n",
    "        B, N, _ = x.shape\n",
    "        \n",
    "        diff = x.unsqueeze(2) - x.unsqueeze(1)  # (B, N, N, 3)\n",
    "        r2 = (diff**2).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # expand features\n",
    "        h_i = h.unsqueeze(2).expand(-1, -1, N, -1)\n",
    "        h_j = h.unsqueeze(1).expand(-1, N, -1, -1)\n",
    "        \n",
    "        t_in = t_emb.unsqueeze(1).unsqueeze(2).expand(B, N, N, -1)\n",
    "        \n",
    "        edge_in = torch.cat([h_i, h_j, r2, t_in], dim=-1)\n",
    "        e_ij = self.edge_mlp(edge_in)\n",
    "        \n",
    "        # coordinate update\n",
    "        w_ij = self.coord_mlp(e_ij)\n",
    "        dx = (w_ij * diff).sum(dim=2)\n",
    "        x = x + dx\n",
    "        \n",
    "        # feature update\n",
    "        m_i = e_ij.sum(dim=2)\n",
    "        h = self.node_mlp(torch.cat([h, m_i], dim=-1))\n",
    "        \n",
    "        return x, h\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# EGNN model: predicts eps (noise), not x0\n",
    "# ===========================\n",
    "class EGNN_Eps(nn.Module):\n",
    "    def __init__(self, n_layers=3, time_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.t_emb = FourierTimeEmbedding(time_dim)\n",
    "        \n",
    "        # node feature init: [x_t, t_emb] -> h\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(3 + time_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EGNNLayer(time_dim=time_dim, hidden_dim=hidden_dim)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 3, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 3)   # -> eps_hat (B,N,3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        B, N, _ = x_t.shape\n",
    "        t_emb = self.t_emb(t)  # (B, time_dim)\n",
    "\n",
    "        # 初始 node feature：你剛問的那句 h_i = MLP([x_t_i, t_emb]) 就在這裡\n",
    "        h = self.node_mlp(torch.cat([\n",
    "            x_t,\n",
    "            t_emb.unsqueeze(1).expand(B, N, -1)\n",
    "        ], dim=-1))   # (B, N, hidden_dim)\n",
    "\n",
    "        x = x_t\n",
    "        for layer in self.layers:\n",
    "            x, h = layer(x, h, t_emb)\n",
    "\n",
    "        # 輸出 eps_hat\n",
    "        eps_hat = self.out_mlp(torch.cat([x, h], dim=-1))\n",
    "        return eps_hat\n",
    "\n",
    "model = EGNN_Eps().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9eeae8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    10 | loss=55289.2188 | Lr=0.000200\n",
      "step    20 | loss=439.6267 | Lr=0.000200\n",
      "step    30 | loss=102.5715 | Lr=0.000200\n",
      "step    40 | loss=240.9147 | Lr=0.000200\n",
      "step    50 | loss=158.1756 | Lr=0.000200\n",
      "step    60 | loss=2.9331 | Lr=0.000200\n",
      "step    70 | loss=2.8929 | Lr=0.000200\n",
      "step    80 | loss=1.7302 | Lr=0.000200\n",
      "step    90 | loss=1.9340 | Lr=0.000200\n",
      "step   100 | loss=390.3542 | Lr=0.000200\n",
      "step   110 | loss=1.3165 | Lr=0.000200\n",
      "step   120 | loss=2.1962 | Lr=0.000200\n",
      "step   130 | loss=1.1954 | Lr=0.000200\n",
      "step   140 | loss=1.6416 | Lr=0.000200\n",
      "step   150 | loss=1.4052 | Lr=0.000200\n",
      "step   160 | loss=0.9893 | Lr=0.000200\n",
      "step   170 | loss=4.6625 | Lr=0.000200\n",
      "step   180 | loss=1.5182 | Lr=0.000200\n",
      "step   190 | loss=3.9708 | Lr=0.000200\n",
      "step   200 | loss=1.0069 | Lr=0.000200\n",
      "step   210 | loss=1.0396 | Lr=0.000200\n",
      "step   220 | loss=0.7841 | Lr=0.000200\n",
      "step   230 | loss=0.8641 | Lr=0.000200\n",
      "step   240 | loss=1.6166 | Lr=0.000200\n",
      "step   250 | loss=0.6267 | Lr=0.000200\n",
      "step   260 | loss=0.4208 | Lr=0.000200\n",
      "step   270 | loss=0.6611 | Lr=0.000200\n",
      "step   280 | loss=0.4319 | Lr=0.000200\n",
      "step   290 | loss=4.1889 | Lr=0.000200\n",
      "step   300 | loss=0.3088 | Lr=0.000200\n",
      "step   310 | loss=0.5323 | Lr=0.000200\n",
      "step   320 | loss=0.3567 | Lr=0.000200\n",
      "step   330 | loss=0.5893 | Lr=0.000200\n",
      "step   340 | loss=0.4550 | Lr=0.000200\n",
      "step   350 | loss=0.3080 | Lr=0.000200\n",
      "step   360 | loss=0.2720 | Lr=0.000200\n",
      "step   370 | loss=1.1595 | Lr=0.000200\n",
      "step   380 | loss=0.3164 | Lr=0.000200\n",
      "step   390 | loss=0.2339 | Lr=0.000200\n",
      "step   400 | loss=0.4090 | Lr=0.000200\n",
      "step   410 | loss=1.0002 | Lr=0.000200\n",
      "step   420 | loss=0.6690 | Lr=0.000200\n",
      "step   430 | loss=0.5055 | Lr=0.000200\n",
      "step   440 | loss=0.2904 | Lr=0.000200\n",
      "step   450 | loss=0.7525 | Lr=0.000200\n",
      "step   460 | loss=0.2915 | Lr=0.000200\n",
      "step   470 | loss=0.2161 | Lr=0.000200\n",
      "step   480 | loss=0.2421 | Lr=0.000200\n",
      "step   490 | loss=0.2318 | Lr=0.000200\n",
      "step   500 | loss=0.2157 | Lr=0.000200\n",
      "step   510 | loss=0.1397 | Lr=0.000200\n",
      "step   520 | loss=0.3032 | Lr=0.000200\n",
      "step   530 | loss=0.1856 | Lr=0.000200\n",
      "step   540 | loss=0.2351 | Lr=0.000200\n",
      "step   550 | loss=0.1375 | Lr=0.000200\n",
      "step   560 | loss=0.0691 | Lr=0.000200\n",
      "step   570 | loss=0.1984 | Lr=0.000200\n",
      "step   580 | loss=0.1634 | Lr=0.000200\n",
      "step   590 | loss=0.1429 | Lr=0.000200\n",
      "step   600 | loss=0.1316 | Lr=0.000200\n",
      "step   610 | loss=0.1890 | Lr=0.000200\n",
      "step   620 | loss=0.1432 | Lr=0.000200\n",
      "step   630 | loss=0.1210 | Lr=0.000200\n",
      "step   640 | loss=0.3168 | Lr=0.000200\n",
      "step   650 | loss=0.2155 | Lr=0.000200\n",
      "step   660 | loss=0.3353 | Lr=0.000200\n",
      "step   670 | loss=0.2653 | Lr=0.000200\n",
      "step   680 | loss=0.1462 | Lr=0.000200\n",
      "step   690 | loss=0.2263 | Lr=0.000200\n",
      "step   700 | loss=0.1800 | Lr=0.000200\n",
      "step   710 | loss=0.2445 | Lr=0.000200\n",
      "step   720 | loss=0.1744 | Lr=0.000200\n",
      "step   730 | loss=0.1697 | Lr=0.000200\n",
      "step   740 | loss=0.1777 | Lr=0.000200\n",
      "step   750 | loss=0.2424 | Lr=0.000200\n",
      "step   760 | loss=0.0709 | Lr=0.000200\n",
      "step   770 | loss=0.1448 | Lr=0.000200\n",
      "step   780 | loss=0.1655 | Lr=0.000200\n",
      "step   790 | loss=0.3556 | Lr=0.000200\n",
      "step   800 | loss=0.0794 | Lr=0.000200\n",
      "step   810 | loss=0.0683 | Lr=0.000200\n",
      "step   820 | loss=0.1873 | Lr=0.000200\n",
      "step   830 | loss=0.1067 | Lr=0.000200\n",
      "step   840 | loss=0.2095 | Lr=0.000200\n",
      "step   850 | loss=0.1811 | Lr=0.000200\n",
      "step   860 | loss=0.2014 | Lr=0.000200\n",
      "step   870 | loss=0.2204 | Lr=0.000200\n",
      "step   880 | loss=0.1337 | Lr=0.000200\n",
      "step   890 | loss=0.1060 | Lr=0.000200\n",
      "step   900 | loss=0.1373 | Lr=0.000200\n",
      "step   910 | loss=0.1062 | Lr=0.000200\n",
      "step   920 | loss=0.2527 | Lr=0.000200\n",
      "step   930 | loss=0.1894 | Lr=0.000200\n",
      "step   940 | loss=0.1655 | Lr=0.000200\n",
      "step   950 | loss=0.1478 | Lr=0.000200\n",
      "step   960 | loss=0.0804 | Lr=0.000200\n",
      "step   970 | loss=0.2101 | Lr=0.000200\n",
      "step   980 | loss=0.0900 | Lr=0.000200\n",
      "step   990 | loss=0.1111 | Lr=0.000200\n",
      "step  1000 | loss=0.2241 | Lr=0.000200\n",
      "step  1010 | loss=0.0709 | Lr=0.000200\n",
      "step  1020 | loss=0.1727 | Lr=0.000200\n",
      "step  1030 | loss=0.2271 | Lr=0.000200\n",
      "step  1040 | loss=0.1525 | Lr=0.000200\n",
      "step  1050 | loss=0.1134 | Lr=0.000200\n",
      "step  1060 | loss=0.1844 | Lr=0.000200\n",
      "step  1070 | loss=0.1670 | Lr=0.000200\n",
      "step  1080 | loss=0.0847 | Lr=0.000200\n",
      "step  1090 | loss=0.2674 | Lr=0.000200\n",
      "step  1100 | loss=0.1220 | Lr=0.000200\n",
      "step  1110 | loss=0.2115 | Lr=0.000200\n",
      "step  1120 | loss=0.1351 | Lr=0.000200\n",
      "step  1130 | loss=0.0983 | Lr=0.000200\n",
      "step  1140 | loss=0.1427 | Lr=0.000200\n",
      "step  1150 | loss=0.0487 | Lr=0.000200\n",
      "step  1160 | loss=0.1240 | Lr=0.000200\n",
      "step  1170 | loss=0.2281 | Lr=0.000200\n",
      "step  1180 | loss=0.0530 | Lr=0.000200\n",
      "step  1190 | loss=0.2313 | Lr=0.000200\n",
      "step  1200 | loss=0.1657 | Lr=0.000200\n",
      "step  1210 | loss=0.3610 | Lr=0.000200\n",
      "step  1220 | loss=0.1634 | Lr=0.000200\n",
      "step  1230 | loss=0.2069 | Lr=0.000200\n",
      "step  1240 | loss=0.1316 | Lr=0.000200\n",
      "step  1250 | loss=0.1598 | Lr=0.000200\n",
      "step  1260 | loss=0.1181 | Lr=0.000200\n",
      "step  1270 | loss=0.0913 | Lr=0.000200\n",
      "step  1280 | loss=0.1460 | Lr=0.000200\n",
      "step  1290 | loss=0.1851 | Lr=0.000200\n",
      "step  1300 | loss=0.1473 | Lr=0.000200\n",
      "step  1310 | loss=0.1053 | Lr=0.000200\n",
      "step  1320 | loss=0.1435 | Lr=0.000200\n",
      "step  1330 | loss=0.1122 | Lr=0.000200\n",
      "step  1340 | loss=0.1919 | Lr=0.000200\n",
      "step  1350 | loss=0.1875 | Lr=0.000200\n",
      "step  1360 | loss=0.2599 | Lr=0.000200\n",
      "step  1370 | loss=0.1602 | Lr=0.000200\n",
      "step  1380 | loss=0.1252 | Lr=0.000200\n",
      "step  1390 | loss=0.1102 | Lr=0.000200\n",
      "step  1400 | loss=0.1933 | Lr=0.000200\n",
      "step  1410 | loss=0.1145 | Lr=0.000200\n",
      "step  1420 | loss=0.2631 | Lr=0.000200\n",
      "step  1430 | loss=0.1401 | Lr=0.000200\n",
      "step  1440 | loss=0.0851 | Lr=0.000200\n",
      "step  1450 | loss=0.2040 | Lr=0.000200\n",
      "step  1460 | loss=0.1153 | Lr=0.000200\n",
      "step  1470 | loss=0.1208 | Lr=0.000200\n",
      "step  1480 | loss=0.1486 | Lr=0.000200\n",
      "step  1490 | loss=0.0486 | Lr=0.000200\n",
      "step  1500 | loss=0.0971 | Lr=0.000200\n",
      "step  1510 | loss=0.1390 | Lr=0.000200\n",
      "step  1520 | loss=0.1147 | Lr=0.000200\n",
      "step  1530 | loss=0.2506 | Lr=0.000200\n",
      "step  1540 | loss=0.0908 | Lr=0.000200\n",
      "step  1550 | loss=0.1860 | Lr=0.000200\n",
      "step  1560 | loss=0.1027 | Lr=0.000200\n",
      "step  1570 | loss=0.0855 | Lr=0.000200\n",
      "step  1580 | loss=0.1287 | Lr=0.000200\n",
      "step  1590 | loss=0.1917 | Lr=0.000200\n",
      "step  1600 | loss=0.2061 | Lr=0.000200\n",
      "step  1610 | loss=0.0569 | Lr=0.000200\n",
      "step  1620 | loss=0.0535 | Lr=0.000200\n",
      "step  1630 | loss=0.2616 | Lr=0.000200\n",
      "step  1640 | loss=0.2052 | Lr=0.000200\n",
      "step  1650 | loss=0.0689 | Lr=0.000200\n",
      "step  1660 | loss=0.0948 | Lr=0.000200\n",
      "step  1670 | loss=0.1382 | Lr=0.000200\n",
      "step  1680 | loss=0.1620 | Lr=0.000200\n",
      "step  1690 | loss=0.1672 | Lr=0.000200\n",
      "step  1700 | loss=0.1903 | Lr=0.000200\n",
      "step  1710 | loss=0.1072 | Lr=0.000200\n",
      "step  1720 | loss=0.1798 | Lr=0.000200\n",
      "step  1730 | loss=0.2547 | Lr=0.000200\n",
      "step  1740 | loss=0.1497 | Lr=0.000200\n",
      "step  1750 | loss=0.2236 | Lr=0.000200\n",
      "step  1760 | loss=0.0854 | Lr=0.000200\n",
      "step  1770 | loss=0.1276 | Lr=0.000200\n",
      "step  1780 | loss=0.0366 | Lr=0.000200\n",
      "step  1790 | loss=0.1905 | Lr=0.000200\n",
      "step  1800 | loss=0.2339 | Lr=0.000200\n",
      "step  1810 | loss=0.1502 | Lr=0.000200\n",
      "step  1820 | loss=0.1247 | Lr=0.000200\n",
      "step  1830 | loss=0.1529 | Lr=0.000200\n",
      "step  1840 | loss=0.0981 | Lr=0.000200\n",
      "step  1850 | loss=0.2066 | Lr=0.000200\n",
      "step  1860 | loss=0.0910 | Lr=0.000200\n",
      "step  1870 | loss=0.1200 | Lr=0.000200\n",
      "step  1880 | loss=0.0685 | Lr=0.000200\n",
      "step  1890 | loss=0.0692 | Lr=0.000200\n",
      "step  1900 | loss=0.0752 | Lr=0.000200\n",
      "step  1910 | loss=0.0468 | Lr=0.000200\n",
      "step  1920 | loss=0.1243 | Lr=0.000200\n",
      "step  1930 | loss=0.1871 | Lr=0.000200\n",
      "step  1940 | loss=0.0357 | Lr=0.000200\n",
      "step  1950 | loss=0.1015 | Lr=0.000200\n",
      "step  1960 | loss=0.1073 | Lr=0.000200\n",
      "step  1970 | loss=0.2472 | Lr=0.000200\n",
      "step  1980 | loss=0.1769 | Lr=0.000200\n",
      "step  1990 | loss=0.0571 | Lr=0.000200\n",
      "step  2000 | loss=0.1786 | Lr=0.000200\n",
      "step  2010 | loss=0.1684 | Lr=0.000200\n",
      "step  2020 | loss=0.0826 | Lr=0.000200\n",
      "step  2030 | loss=0.1087 | Lr=0.000200\n",
      "step  2040 | loss=0.0610 | Lr=0.000200\n",
      "step  2050 | loss=0.2261 | Lr=0.000200\n",
      "step  2060 | loss=0.1571 | Lr=0.000200\n",
      "step  2070 | loss=0.1281 | Lr=0.000200\n",
      "step  2080 | loss=0.2115 | Lr=0.000200\n",
      "step  2090 | loss=0.2004 | Lr=0.000200\n",
      "step  2100 | loss=0.1943 | Lr=0.000200\n",
      "step  2110 | loss=0.3838 | Lr=0.000200\n",
      "step  2120 | loss=0.0903 | Lr=0.000200\n",
      "step  2130 | loss=0.1741 | Lr=0.000200\n",
      "step  2140 | loss=0.0915 | Lr=0.000200\n",
      "step  2150 | loss=0.2398 | Lr=0.000200\n",
      "step  2160 | loss=0.3104 | Lr=0.000200\n",
      "step  2170 | loss=0.0493 | Lr=0.000200\n",
      "step  2180 | loss=0.1355 | Lr=0.000200\n",
      "step  2190 | loss=0.0852 | Lr=0.000200\n",
      "step  2200 | loss=0.1005 | Lr=0.000200\n",
      "step  2210 | loss=0.1482 | Lr=0.000200\n",
      "step  2220 | loss=0.2286 | Lr=0.000200\n",
      "step  2230 | loss=0.0660 | Lr=0.000200\n",
      "step  2240 | loss=0.1793 | Lr=0.000200\n",
      "step  2250 | loss=0.1405 | Lr=0.000200\n",
      "step  2260 | loss=0.1297 | Lr=0.000200\n",
      "step  2270 | loss=0.1491 | Lr=0.000200\n",
      "step  2280 | loss=0.1081 | Lr=0.000200\n",
      "step  2290 | loss=0.3079 | Lr=0.000200\n",
      "step  2300 | loss=0.2501 | Lr=0.000200\n",
      "step  2310 | loss=0.1180 | Lr=0.000200\n",
      "step  2320 | loss=0.1013 | Lr=0.000200\n",
      "step  2330 | loss=0.1430 | Lr=0.000200\n",
      "step  2340 | loss=0.1091 | Lr=0.000200\n",
      "step  2350 | loss=0.1936 | Lr=0.000200\n",
      "step  2360 | loss=0.1588 | Lr=0.000200\n",
      "step  2370 | loss=0.1527 | Lr=0.000200\n",
      "step  2380 | loss=0.2159 | Lr=0.000200\n",
      "step  2390 | loss=0.1078 | Lr=0.000200\n",
      "step  2400 | loss=0.1353 | Lr=0.000200\n",
      "step  2410 | loss=0.2201 | Lr=0.000200\n",
      "step  2420 | loss=0.1001 | Lr=0.000200\n",
      "step  2430 | loss=0.1864 | Lr=0.000200\n",
      "step  2440 | loss=0.2296 | Lr=0.000200\n",
      "step  2450 | loss=0.1381 | Lr=0.000200\n",
      "step  2460 | loss=0.0866 | Lr=0.000200\n",
      "step  2470 | loss=0.0554 | Lr=0.000200\n",
      "step  2480 | loss=0.1357 | Lr=0.000200\n",
      "step  2490 | loss=0.0889 | Lr=0.000200\n",
      "step  2500 | loss=0.1179 | Lr=0.000200\n",
      "step  2510 | loss=0.2430 | Lr=0.000200\n",
      "step  2520 | loss=0.1286 | Lr=0.000200\n",
      "step  2530 | loss=0.1787 | Lr=0.000200\n",
      "step  2540 | loss=0.2771 | Lr=0.000200\n",
      "step  2550 | loss=0.1295 | Lr=0.000200\n",
      "step  2560 | loss=0.0649 | Lr=0.000200\n",
      "step  2570 | loss=0.0884 | Lr=0.000200\n",
      "step  2580 | loss=0.1420 | Lr=0.000200\n",
      "step  2590 | loss=0.0920 | Lr=0.000200\n",
      "step  2600 | loss=0.1276 | Lr=0.000200\n",
      "step  2610 | loss=0.1587 | Lr=0.000200\n",
      "step  2620 | loss=0.1159 | Lr=0.000200\n",
      "step  2630 | loss=0.1955 | Lr=0.000200\n",
      "step  2640 | loss=0.1446 | Lr=0.000200\n",
      "step  2650 | loss=0.1192 | Lr=0.000200\n",
      "step  2660 | loss=0.1394 | Lr=0.000200\n",
      "step  2670 | loss=0.0710 | Lr=0.000200\n",
      "step  2680 | loss=0.0778 | Lr=0.000200\n",
      "step  2690 | loss=0.1058 | Lr=0.000200\n",
      "step  2700 | loss=0.1388 | Lr=0.000200\n",
      "step  2710 | loss=0.2734 | Lr=0.000200\n",
      "step  2720 | loss=0.1139 | Lr=0.000200\n",
      "step  2730 | loss=0.1247 | Lr=0.000200\n",
      "step  2740 | loss=0.2267 | Lr=0.000200\n",
      "step  2750 | loss=0.1027 | Lr=0.000200\n",
      "step  2760 | loss=0.0948 | Lr=0.000200\n",
      "step  2770 | loss=0.2019 | Lr=0.000200\n",
      "step  2780 | loss=0.0809 | Lr=0.000200\n",
      "step  2790 | loss=0.1810 | Lr=0.000200\n",
      "step  2800 | loss=0.0907 | Lr=0.000200\n",
      "step  2810 | loss=0.2114 | Lr=0.000200\n",
      "step  2820 | loss=0.2748 | Lr=0.000200\n",
      "step  2830 | loss=0.1538 | Lr=0.000200\n",
      "step  2840 | loss=0.0470 | Lr=0.000200\n",
      "step  2850 | loss=0.2404 | Lr=0.000200\n",
      "step  2860 | loss=0.1859 | Lr=0.000200\n",
      "step  2870 | loss=0.0848 | Lr=0.000200\n",
      "step  2880 | loss=0.0884 | Lr=0.000200\n",
      "step  2890 | loss=0.0528 | Lr=0.000200\n",
      "step  2900 | loss=0.0736 | Lr=0.000200\n",
      "step  2910 | loss=0.0762 | Lr=0.000200\n",
      "step  2920 | loss=0.1634 | Lr=0.000200\n",
      "step  2930 | loss=0.2088 | Lr=0.000200\n",
      "step  2940 | loss=0.2037 | Lr=0.000200\n",
      "step  2950 | loss=0.0782 | Lr=0.000200\n",
      "step  2960 | loss=0.0620 | Lr=0.000200\n",
      "step  2970 | loss=0.1185 | Lr=0.000200\n",
      "step  2980 | loss=0.1083 | Lr=0.000200\n",
      "step  2990 | loss=0.1519 | Lr=0.000200\n",
      "step  3000 | loss=0.1326 | Lr=0.000200\n",
      "step  3010 | loss=0.1013 | Lr=0.000200\n",
      "step  3020 | loss=0.1452 | Lr=0.000200\n",
      "step  3030 | loss=0.1484 | Lr=0.000200\n",
      "step  3040 | loss=0.2058 | Lr=0.000200\n",
      "step  3050 | loss=0.0758 | Lr=0.000200\n",
      "step  3060 | loss=0.0524 | Lr=0.000200\n",
      "step  3070 | loss=0.2099 | Lr=0.000200\n",
      "step  3080 | loss=0.0475 | Lr=0.000200\n",
      "step  3090 | loss=0.0409 | Lr=0.000200\n",
      "step  3100 | loss=0.0859 | Lr=0.000200\n",
      "step  3110 | loss=0.1883 | Lr=0.000200\n",
      "step  3120 | loss=0.0878 | Lr=0.000200\n",
      "step  3130 | loss=0.0859 | Lr=0.000200\n",
      "step  3140 | loss=0.0572 | Lr=0.000200\n",
      "step  3150 | loss=0.0882 | Lr=0.000200\n",
      "step  3160 | loss=0.1904 | Lr=0.000200\n",
      "step  3170 | loss=0.0914 | Lr=0.000200\n",
      "step  3180 | loss=0.1549 | Lr=0.000200\n",
      "step  3190 | loss=0.1866 | Lr=0.000200\n",
      "step  3200 | loss=0.0764 | Lr=0.000200\n",
      "step  3210 | loss=0.1819 | Lr=0.000200\n",
      "step  3220 | loss=0.1227 | Lr=0.000200\n",
      "step  3230 | loss=0.2436 | Lr=0.000200\n",
      "step  3240 | loss=0.0866 | Lr=0.000200\n",
      "step  3250 | loss=0.2404 | Lr=0.000200\n",
      "step  3260 | loss=0.0998 | Lr=0.000200\n",
      "step  3270 | loss=0.0994 | Lr=0.000200\n",
      "step  3280 | loss=0.2441 | Lr=0.000200\n",
      "step  3290 | loss=0.1145 | Lr=0.000200\n",
      "step  3300 | loss=0.0984 | Lr=0.000200\n",
      "step  3310 | loss=0.0956 | Lr=0.000200\n",
      "step  3320 | loss=0.0918 | Lr=0.000200\n",
      "step  3330 | loss=0.1062 | Lr=0.000200\n",
      "step  3340 | loss=0.1287 | Lr=0.000200\n",
      "step  3350 | loss=0.1112 | Lr=0.000200\n",
      "step  3360 | loss=0.1645 | Lr=0.000200\n",
      "step  3370 | loss=0.0431 | Lr=0.000200\n",
      "step  3380 | loss=0.0990 | Lr=0.000200\n",
      "step  3390 | loss=0.1452 | Lr=0.000200\n",
      "step  3400 | loss=0.0903 | Lr=0.000200\n",
      "step  3410 | loss=0.0614 | Lr=0.000200\n",
      "step  3420 | loss=0.1639 | Lr=0.000200\n",
      "step  3430 | loss=0.1403 | Lr=0.000200\n",
      "step  3440 | loss=0.0511 | Lr=0.000200\n",
      "step  3450 | loss=0.1570 | Lr=0.000200\n",
      "step  3460 | loss=0.0782 | Lr=0.000200\n",
      "step  3470 | loss=0.0646 | Lr=0.000200\n",
      "step  3480 | loss=0.2041 | Lr=0.000200\n",
      "step  3490 | loss=0.0403 | Lr=0.000200\n",
      "step  3500 | loss=0.0674 | Lr=0.000200\n",
      "step  3510 | loss=0.1262 | Lr=0.000200\n",
      "step  3520 | loss=0.1370 | Lr=0.000200\n",
      "step  3530 | loss=0.0449 | Lr=0.000200\n",
      "step  3540 | loss=0.2001 | Lr=0.000200\n",
      "step  3550 | loss=0.1994 | Lr=0.000200\n",
      "step  3560 | loss=0.1351 | Lr=0.000200\n",
      "step  3570 | loss=0.0646 | Lr=0.000200\n",
      "step  3580 | loss=0.2403 | Lr=0.000200\n",
      "step  3590 | loss=0.1323 | Lr=0.000200\n",
      "step  3600 | loss=0.1000 | Lr=0.000200\n",
      "step  3610 | loss=0.1512 | Lr=0.000200\n",
      "step  3620 | loss=0.1554 | Lr=0.000200\n",
      "step  3630 | loss=0.1513 | Lr=0.000200\n",
      "step  3640 | loss=0.0862 | Lr=0.000200\n",
      "step  3650 | loss=0.0490 | Lr=0.000200\n",
      "step  3660 | loss=0.1018 | Lr=0.000200\n",
      "step  3670 | loss=0.0808 | Lr=0.000200\n",
      "step  3680 | loss=0.1819 | Lr=0.000200\n",
      "step  3690 | loss=0.0833 | Lr=0.000200\n",
      "step  3700 | loss=0.1291 | Lr=0.000200\n",
      "step  3710 | loss=0.2572 | Lr=0.000200\n",
      "step  3720 | loss=0.1142 | Lr=0.000200\n",
      "step  3730 | loss=0.1328 | Lr=0.000200\n",
      "step  3740 | loss=0.2515 | Lr=0.000200\n",
      "step  3750 | loss=0.1153 | Lr=0.000200\n",
      "step  3760 | loss=0.2004 | Lr=0.000200\n",
      "step  3770 | loss=0.1056 | Lr=0.000200\n",
      "step  3780 | loss=0.1971 | Lr=0.000200\n",
      "step  3790 | loss=0.1107 | Lr=0.000200\n",
      "step  3800 | loss=0.1075 | Lr=0.000200\n",
      "step  3810 | loss=0.1428 | Lr=0.000200\n",
      "step  3820 | loss=0.1135 | Lr=0.000200\n",
      "step  3830 | loss=0.1011 | Lr=0.000200\n",
      "step  3840 | loss=0.0874 | Lr=0.000200\n",
      "step  3850 | loss=0.0568 | Lr=0.000200\n",
      "step  3860 | loss=0.0960 | Lr=0.000200\n",
      "step  3870 | loss=0.1468 | Lr=0.000200\n",
      "step  3880 | loss=0.1573 | Lr=0.000200\n",
      "step  3890 | loss=0.1632 | Lr=0.000200\n",
      "step  3900 | loss=0.1189 | Lr=0.000200\n",
      "step  3910 | loss=0.0867 | Lr=0.000200\n",
      "step  3920 | loss=0.0313 | Lr=0.000200\n",
      "step  3930 | loss=0.3245 | Lr=0.000200\n",
      "step  3940 | loss=0.3194 | Lr=0.000200\n",
      "step  3950 | loss=0.1712 | Lr=0.000200\n",
      "step  3960 | loss=0.1211 | Lr=0.000200\n",
      "step  3970 | loss=0.1271 | Lr=0.000200\n",
      "step  3980 | loss=0.2205 | Lr=0.000200\n",
      "step  3990 | loss=0.1046 | Lr=0.000200\n",
      "step  4000 | loss=0.0845 | Lr=0.000200\n",
      "step  4010 | loss=0.1812 | Lr=0.000200\n",
      "step  4020 | loss=0.2013 | Lr=0.000200\n",
      "step  4030 | loss=0.2853 | Lr=0.000200\n",
      "step  4040 | loss=0.1439 | Lr=0.000200\n",
      "step  4050 | loss=0.2099 | Lr=0.000200\n",
      "step  4060 | loss=0.1584 | Lr=0.000200\n",
      "step  4070 | loss=0.1435 | Lr=0.000200\n",
      "step  4080 | loss=0.0982 | Lr=0.000200\n",
      "step  4090 | loss=0.0944 | Lr=0.000200\n",
      "step  4100 | loss=0.1009 | Lr=0.000200\n",
      "step  4110 | loss=0.2056 | Lr=0.000200\n",
      "step  4120 | loss=0.1137 | Lr=0.000200\n",
      "step  4130 | loss=0.1039 | Lr=0.000200\n",
      "step  4140 | loss=0.1970 | Lr=0.000200\n",
      "step  4150 | loss=0.2497 | Lr=0.000200\n",
      "step  4160 | loss=0.1018 | Lr=0.000200\n",
      "step  4170 | loss=0.1371 | Lr=0.000200\n",
      "step  4180 | loss=0.1674 | Lr=0.000200\n",
      "step  4190 | loss=0.1304 | Lr=0.000200\n",
      "step  4200 | loss=0.0777 | Lr=0.000200\n",
      "step  4210 | loss=0.0305 | Lr=0.000200\n",
      "step  4220 | loss=0.1366 | Lr=0.000200\n",
      "step  4230 | loss=0.1278 | Lr=0.000200\n",
      "step  4240 | loss=0.1858 | Lr=0.000200\n",
      "step  4250 | loss=0.0812 | Lr=0.000200\n",
      "step  4260 | loss=0.2296 | Lr=0.000200\n",
      "step  4270 | loss=0.1038 | Lr=0.000200\n",
      "step  4280 | loss=0.0901 | Lr=0.000200\n",
      "step  4290 | loss=0.1404 | Lr=0.000200\n",
      "step  4300 | loss=0.1117 | Lr=0.000200\n",
      "step  4310 | loss=0.1904 | Lr=0.000200\n",
      "step  4320 | loss=0.1309 | Lr=0.000200\n",
      "step  4330 | loss=0.1957 | Lr=0.000200\n",
      "step  4340 | loss=0.2764 | Lr=0.000200\n",
      "step  4350 | loss=0.1800 | Lr=0.000200\n",
      "step  4360 | loss=0.0432 | Lr=0.000200\n",
      "step  4370 | loss=0.1106 | Lr=0.000200\n",
      "step  4380 | loss=0.2179 | Lr=0.000200\n",
      "step  4390 | loss=0.2357 | Lr=0.000200\n",
      "step  4400 | loss=0.1695 | Lr=0.000200\n",
      "step  4410 | loss=0.1349 | Lr=0.000200\n",
      "step  4420 | loss=0.1331 | Lr=0.000200\n",
      "step  4430 | loss=0.0806 | Lr=0.000200\n",
      "step  4440 | loss=0.1956 | Lr=0.000200\n",
      "step  4450 | loss=0.1359 | Lr=0.000200\n",
      "step  4460 | loss=0.1468 | Lr=0.000200\n",
      "step  4470 | loss=0.1759 | Lr=0.000200\n",
      "step  4480 | loss=0.0822 | Lr=0.000200\n",
      "step  4490 | loss=0.0646 | Lr=0.000200\n",
      "step  4500 | loss=0.1509 | Lr=0.000200\n",
      "step  4510 | loss=0.2231 | Lr=0.000200\n",
      "step  4520 | loss=0.2017 | Lr=0.000200\n",
      "step  4530 | loss=0.1000 | Lr=0.000200\n",
      "step  4540 | loss=0.1148 | Lr=0.000200\n",
      "step  4550 | loss=0.0761 | Lr=0.000200\n",
      "step  4560 | loss=0.1026 | Lr=0.000200\n",
      "step  4570 | loss=0.2118 | Lr=0.000200\n",
      "step  4580 | loss=0.1235 | Lr=0.000200\n",
      "step  4590 | loss=0.2201 | Lr=0.000200\n",
      "step  4600 | loss=0.1265 | Lr=0.000200\n",
      "step  4610 | loss=0.2816 | Lr=0.000200\n",
      "step  4620 | loss=0.1167 | Lr=0.000200\n",
      "step  4630 | loss=0.1669 | Lr=0.000200\n",
      "step  4640 | loss=0.1649 | Lr=0.000200\n",
      "step  4650 | loss=0.2352 | Lr=0.000200\n",
      "step  4660 | loss=0.1617 | Lr=0.000200\n",
      "step  4670 | loss=0.1942 | Lr=0.000200\n",
      "step  4680 | loss=0.0914 | Lr=0.000200\n",
      "step  4690 | loss=0.1080 | Lr=0.000200\n",
      "step  4700 | loss=0.1127 | Lr=0.000200\n",
      "step  4710 | loss=0.0655 | Lr=0.000200\n",
      "step  4720 | loss=0.0818 | Lr=0.000200\n",
      "step  4730 | loss=0.1537 | Lr=0.000200\n",
      "step  4740 | loss=0.1609 | Lr=0.000200\n",
      "step  4750 | loss=0.1766 | Lr=0.000200\n",
      "step  4760 | loss=0.2081 | Lr=0.000200\n",
      "step  4770 | loss=0.1678 | Lr=0.000200\n",
      "step  4780 | loss=0.0990 | Lr=0.000200\n",
      "step  4790 | loss=0.0780 | Lr=0.000200\n",
      "step  4800 | loss=0.1820 | Lr=0.000200\n",
      "step  4810 | loss=0.0898 | Lr=0.000200\n",
      "step  4820 | loss=0.0230 | Lr=0.000200\n",
      "step  4830 | loss=0.1412 | Lr=0.000200\n",
      "step  4840 | loss=0.1628 | Lr=0.000200\n",
      "step  4850 | loss=0.1706 | Lr=0.000200\n",
      "step  4860 | loss=0.1742 | Lr=0.000200\n",
      "step  4870 | loss=0.0584 | Lr=0.000200\n",
      "step  4880 | loss=0.0543 | Lr=0.000200\n",
      "step  4890 | loss=0.0896 | Lr=0.000200\n",
      "step  4900 | loss=0.2771 | Lr=0.000200\n",
      "step  4910 | loss=0.1046 | Lr=0.000200\n",
      "step  4920 | loss=0.2141 | Lr=0.000200\n",
      "step  4930 | loss=0.0994 | Lr=0.000200\n",
      "step  4940 | loss=0.1835 | Lr=0.000200\n",
      "step  4950 | loss=0.2527 | Lr=0.000200\n",
      "step  4960 | loss=0.1140 | Lr=0.000200\n",
      "step  4970 | loss=0.2145 | Lr=0.000200\n",
      "step  4980 | loss=0.2880 | Lr=0.000200\n",
      "step  4990 | loss=0.1459 | Lr=0.000200\n",
      "step  5000 | loss=0.1198 | Lr=0.000200\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Loss: predict eps (noise)\n",
    "# ===========================\n",
    "def loss_eps(model, batch_size):\n",
    "    x0 = sample_x0_cloud(batch_size)   # (B,N,3)\n",
    "\n",
    "    # 避免太小的 t（小 noise 很不穩），取 0.2T ~ T\n",
    "    t = torch.randint(int(0.2 * T), T, (batch_size,), device=device)\n",
    "\n",
    "    alpha_bar_t = alpha_bar[t].view(-1,1,1)\n",
    "    sigma_t     = sigmas[t].view(-1,1,1)\n",
    "\n",
    "    eps = torch.randn_like(x0)\n",
    "    x_t = torch.sqrt(alpha_bar_t)*x0 + sigma_t*eps\n",
    "\n",
    "    eps_hat = model(x_t, t)\n",
    "\n",
    "    return ((eps_hat - eps)**2).mean()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Training loop\n",
    "# ===========================\n",
    "batch_size = 8\n",
    "lr = 2e-4\n",
    "num_steps = 5000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=200,\n",
    "    cooldown=100,\n",
    "    min_lr=1e-8\n",
    ")\n",
    "loss_history_1 = []\n",
    "\n",
    "model.train()\n",
    "for step in range(1, num_steps+1):\n",
    "    loss = loss_eps(model, batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history_1.append(loss.item())\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {step:5d} | loss={loss.item():.4f} | Lr={current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f129e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAARilJREFUeJzt3XlclXXe//H3QeAAsYiikIpI7humuGFTaqFoDolZOVbj0j7plGO2WLlPoZWTlqV1N8nknVn2S2pyJRTNpUzTcotyxsQKMFNARPEA1+8P73PqxCJ4gOsIr+fjce4H53t9r+v6HM5nvHl3bRbDMAwBAAAAgAs8zC4AAAAAwOWPYAEAAADAZQQLAAAAAC4jWAAAAABwGcECAAAAgMsIFgAAAABcRrAAAAAA4DKCBQAAAACXESwAAAAAuIxgAQDQuHHj1KpVq0tad+bMmbJYLNVbEADgskOwAFAvJSUlyWKxOF4+Pj5q1qyZ4uLi9NJLL+n06dNlrrd161YNHTpUzZs3l4+Pj1q2bKn4+HgtX77caZ59u/fcc0+Z23nqqaccc06cOFFunb+tsaJXWlraJf8uLmfjxo2Tv7+/2WVU2qpVqzR06FCFhITI29tbzZo102233aaNGzeaXRoAuMxiGIZhdhEAUNuSkpI0fvx4zZ49W5GRkbLZbMrKylJaWppSUlLUsmVLffTRR4qKinKss3LlSo0aNUpXX321/vSnPyk4OFhHjhzRli1b5OXlpU2bNjnm2sOKj4+PsrOz5e3t7bT/q666SpmZmTp37px+/vlnhYSElFnn//7v/zq9f+utt5SSkqJly5Y5jQ8aNEihoaGX/Puw2WwqKSmR1Wqt8rpFRUUqKiqSj4/PJe//Uo0bN07vv/++8vPza33fVWEYhu666y4lJSWpe/fuuuWWWxQWFqbMzEytWrVKu3fv1rZt29SvXz+zSwWAS2cAQD20dOlSQ5LxxRdflFqWmppq+Pr6GhEREUZBQYFjvFOnTkbnzp2NwsLCUutkZ2c7vZdkJCQkGB4eHkZycrLTsm3bthmSjJEjRxqSjJ9//rnSdU+YMMGozD/dZ86cqfQ2L2djx441rrjiCrPLuKjnn3/ekGRMmjTJKCkpKbX8rbfeMj7//HOX91NSUuLUswBQmzgVCgB+5/rrr9e0adN09OhRpyMG//nPf9SrV69SRx8kqWnTpqXGmjdvruuuu67UaVJvv/22unbtqi5dulRLvQMGDFCXLl20e/duXXfddfLz89OTTz4pSfrwww81bNgwNWvWTFarVa1bt9acOXNUXFzstI3fX2Px/fffy2Kx6IUXXtDrr7+u1q1by2q1qlevXvriiy+c1i3rGguLxaKJEycqOTlZXbp0kdVqVefOnbVu3bpS9aelpalnz57y8fFR69at9dprr1X7dRsrV65UdHS0fH19FRISojvvvFM//vij05ysrCyNHz9eLVq0kNVq1ZVXXqnhw4fr+++/d8zZtWuX4uLiFBISIl9fX0VGRuquu+6qcN9nz55VYmKiOnTooBdeeKHMz/XnP/9ZvXv3llT+NSv20/d+W0+rVq30xz/+UevXr1fPnj3l6+ur1157TV26dNHAgQNLbaOkpETNmzfXLbfc4jS2YMECde7cWT4+PgoNDdX999+vU6dOVfi5AOD3PM0uAADc0Z///Gc9+eST2rBhg+69915JUkREhFJTU/XDDz+oRYsWldrO7bffrocfflj5+fny9/dXUVGRVq5cqcmTJ+vcuXPVVu8vv/yioUOH6k9/+pPuvPNOx2lRSUlJ8vf31+TJk+Xv76+NGzdq+vTpysvL0/PPP3/R7S5fvlynT5/W/fffL4vFoueee04333yz/vvf/8rLy6vCdbdu3aoPPvhADz74oAICAvTSSy9p5MiRysjIUOPGjSVJe/bs0ZAhQ3TllVdq1qxZKi4u1uzZs9WkSRPXfyn/x37aW69evZSYmKjs7GwtXLhQ27Zt0549e9SwYUNJ0siRI3XgwAH99a9/VatWrXT8+HGlpKQoIyPD8X7w4MFq0qSJnnjiCTVs2FDff/+9Pvjgg4v+Hk6ePKlJkyapQYMG1fa57NLT0zV69Gjdf//9uvfee9W+fXuNGjVKM2fOVFZWlsLCwpxq+emnn/SnP/3JMXb//fc7fkcPPfSQjhw5okWLFmnPnj3atm3bRb9nAHAw+5AJAJiholOh7IKCgozu3bs73v/zn/80JBne3t7GwIEDjWnTphmffvqpUVxcXGpdScaECROMkydPGt7e3sayZcsMwzCM1atXGxaLxfj++++NGTNmVMupUP379zckGUuWLCk1v6zTYu6//37Dz8/POHfunGNs7NixRkREhOP9kSNHDElG48aNjZMnTzrGP/zwQ0OS8e9//9sxZv8cv//83t7exuHDhx1jX331lSHJePnllx1j8fHxhp+fn/Hjjz86xr777jvD09OzUqd8XexUqPPnzxtNmzY1unTpYpw9e9Yx/vHHHxuSjOnTpxuGYRinTp0yJBnPP/98udtatWrVRXumLAsXLjQkGatWrarU/LJ+n4bxa88eOXLEMRYREWFIMtatW+c0Nz09vdTv2jAM48EHHzT8/f0dffHpp58akoy3337bad66devKHAeAinAqFACUw9/f3+nuUHfddZfWrVunAQMGaOvWrZozZ46uvfZatW3bVtu3by9zG8HBwRoyZIjeeecdSReOAPTr108RERHVWqvVatX48eNLjfv6+jp+Pn36tE6cOKFrr71WBQUF+uabby663VGjRik4ONjx/tprr5Uk/fe//73ourGxsWrdurXjfVRUlAIDAx3rFhcX65NPPlFCQoKaNWvmmNemTRsNHTr0otuvjF27dun48eN68MEHnS4uHzZsmDp06KDVq1dLuvB78vb2VlpaWrmnANmPbHz88cey2WyVriEvL0+SFBAQcImfomKRkZGKi4tzGmvXrp2uvvpqvfvuu46x4uJivf/++4qPj3f0xcqVKxUUFKRBgwbpxIkTjld0dLT8/f2dbkgAABdTr4PFli1bFB8fr2bNmslisSg5ObnK21i/fr369u2rgIAANWnSRCNHjnQ6/xXA5Ss/P7/UH4NxcXFav369cnJytGXLFk2YMEFHjx7VH//4Rx0/frzM7dx+++2OU2qSk5N1++23V3utzZs3L/PajwMHDmjEiBEKCgpSYGCgmjRpojvvvFOSlJube9HttmzZ0um9PWRU5vz7369rX9++7vHjx3X27Fm1adOm1Lyyxi7F0aNHJUnt27cvtaxDhw6O5VarVfPmzdPatWsVGhqq6667Ts8995yysrIc8/v376+RI0dq1qxZCgkJ0fDhw7V06VIVFhZWWENgYKAklXsLY1dFRkaWOT5q1Cht27bNcS1JWlqajh8/rlGjRjnmfPfdd8rNzVXTpk3VpEkTp1d+fn65PQ0AZanXweLMmTPq1q2bXnnllUta/8iRIxo+fLiuv/567d27V+vXr9eJEyd08803V3OlAGrbDz/8oNzc3HL/wPXz89O1116rRYsW6emnn9apU6e0du3aMufedNNNslqtGjt2rAoLC3XbbbdVe72/PTJhl5OTo/79++urr77S7Nmz9e9//1spKSmaN2+epAsX7V5MedcEGJW4U7kr65ph0qRJ+vbbb5WYmCgfHx9NmzZNHTt21J49eyRduCD9/fff144dOzRx4kT9+OOPuuuuuxQdHV3h7W47dOggSdq3b1+l6ijvovXfX3BvV9Z3L10IFoZhaOXKlZKk9957T0FBQRoyZIhjTklJiZo2baqUlJQyX7Nnz65UzQAg1fNgMXToUP3973/XiBEjylxeWFioKVOmqHnz5rriiivUp08fp4dQ7d69W8XFxfr73/+u1q1bq0ePHpoyZYr27t1bpcPkANyP/TkRvz/FpCw9e/aUJGVmZpa53NfXVwkJCUpLS9OgQYPKfWZFdUtLS9Mvv/yipKQkPfzww/rjH/+o2NhYp1ObzNS0aVP5+Pjo8OHDpZaVNXYp7Kecpaenl1qWnp5e6pS01q1b65FHHtGGDRu0f/9+nT9/XvPnz3ea07dvXz3zzDPatWuX3n77bR04cEArVqwot4Y//OEPCg4O1jvvvFNuOPgt+/eTk5PjNG4/ulJZkZGR6t27t959910VFRXpgw8+UEJCgtOzSlq3bq1ffvlF11xzjWJjY0u9unXrVqV9Aqjf6nWwuJiJEydqx44dWrFihb7++mvdeuutGjJkiL777jtJUnR0tDw8PLR06VIVFxcrNzdXy5YtU2xsLHfRAC5jGzdu1Jw5cxQZGak77rjDMZ6amlrm/DVr1kgq+3QbuylTpmjGjBmaNm1a9RZbAfsRg98eITh//rxeffXVWquhIg0aNFBsbKySk5P1008/OcYPHz5c7tGfqurZs6eaNm2qJUuWOJ2ytHbtWh06dEjDhg2TJBUUFJS6S1fr1q0VEBDgWO/UqVOljrZcffXVklTh6VB+fn56/PHHdejQIT3++ONlHrH53//9X+3cudOxX+nC6bp2Z86c0b/+9a/KfmyHUaNG6bPPPtObb76pEydOOJ0GJUm33XabiouLNWfOnFLrFhUVlQo3AFARbjdbjoyMDC1dulQZGRmOiwqnTJmidevWaenSpXr22WcVGRmpDRs26LbbbtP999+v4uJixcTEOP7IAOD+1q5dq2+++UZFRUXKzs7Wxo0blZKSooiICH300UdOF/wOHz5ckZGRio+PV+vWrXXmzBl98skn+ve//61evXopPj6+3P1069at1v/rb79+/RQcHKyxY8fqoYceksVi0bJly9zqVKSZM2dqw4YNuuaaa/SXv/xFxcXFWrRokbp06aK9e/dWahs2m01///vfS403atRIDz74oObNm6fx48erf//+Gj16tON2s61atdLf/vY3SdK3336rG264Qbfddps6deokT09PrVq1StnZ2Y5bs/7rX//Sq6++qhEjRqh169Y6ffq0/ud//keBgYG68cYbK6zx0Ucf1YEDBzR//nxt2rTJ8eTtrKwsJScna+fOnY4bAAwePFgtW7bU3XffrUcffVQNGjTQm2++qSZNmigjI6MKv90LwWHKlCmaMmWKGjVqpNjYWKfl/fv31/3336/ExETt3btXgwcPlpeXl7777jutXLlSCxcudHrmBQBUhGBRjn379qm4uFjt2rVzGi8sLHTcfz0rK0v33nuvxo4dq9GjR+v06dOaPn26brnlFqWkpFTrw50A1Izp06dLkry9vdWoUSN17dpVCxYs0Pjx40tduP3GG2/oww8/1HvvvaeffvpJhmHoqquu0lNPPaXHH39cnp7u9U9q48aN9fHHH+uRRx7R008/reDgYN1555264YYbKnWKV22Ijo7W2rVrNWXKFE2bNk3h4eGaPXu2Dh06VKm7VkkXjsKUdSSodevWevDBBzVu3Dj5+flp7ty5evzxx3XFFVdoxIgRmjdvnuNOT+Hh4Ro9erRSU1O1bNkyeXp6qkOHDnrvvfc0cuRISRf+CN+5c6dWrFih7OxsBQUFqXfv3nr77bfLvYDazsPDQ2+99ZaGDx+u119/XS+88ILy8vLUpEkTx4XiMTExkiQvLy+tWrVKDz74oKZNm6awsDBNmjRJwcHBZd75qyItWrRQv379tG3bNt1zzz1lHk1fsmSJoqOj9dprr+nJJ5+Up6enWrVqpTvvvFPXXHNNlfYHoH6zGO70n65MZLFYtGrVKiUkJEiS3n33Xd1xxx06cOBAqQsQ/f39FRYWpmnTpmndunVOT6H94YcfFB4erh07dqhv3761+REAoM5ISEjQgQMHHKeeAgDcn3v95zU30r17dxUXF+v48eOO+7b/XkFBgTw8nC9TsYeQytxtBQAgnT171unORt99953WrFmjsWPHmlgVAKCq6nWwyM/Pd7rzyJEjR7R37141atRI7dq10x133KExY8Zo/vz56t69u37++WelpqYqKipKw4YN07Bhw/Tiiy9q9uzZjlOhnnzySUVERKh79+4mfjIAuHxcddVVGjdunK666iodPXpUixcvlre3tx577DGzSwMAVEG9PhUqLS1NAwcOLDU+duxYJSUlOS4IfOutt/Tjjz8qJCREffv21axZs9S1a1dJ0ooVK/Tcc8/p22+/lZ+fn2JiYjRv3jzHfcsBABUbP368Nm3apKysLFmtVsXExOjZZ59Vjx49zC4NAFAF9TpYAAAAAKgePMcCAAAAgMsIFgAAAABcVu8u3i4pKdFPP/2kgIAAnjMBAAAAVMAwDJ0+fVrNmjUrdTfU36t3weKnn35SeHi42WUAAAAAl41jx46pRYsWFc6pd8HC/iTdY8eOKTAw0JQabDabNmzYoMGDB5f5FFTUH/QCJPoAv6IXYEcvQHKPPsjLy1N4eLjjb+iK1LtgYT/9KTAw0NRg4efnp8DAQP6xqOfoBUj0AX5FL8COXoDkXn1QmUsIuHgbAAAAgMsIFgAAAABcRrAAAAAA4DKCBQAAAACXESwAAAAAuIxgAQAAAMBlBAsAAAAALiNYAAAAAHAZwQIAAACAywgWAAAAAFzmaXYB9c0v+YUavmirzp5toBtvNLsaAAAAoHoQLGpZsWHoh5xzsphdCAAAAFCNOBUKAAAAgMsIFgAAAABcZmqwWLx4saKiohQYGKjAwEDFxMRo7dq1Fa6zcuVKdejQQT4+PuratavWrFlTS9UCAAAAKI+pwaJFixaaO3eudu/erV27dun666/X8OHDdeDAgTLnb9++XaNHj9bdd9+tPXv2KCEhQQkJCdq/f38tVw4AAADgt0wNFvHx8brxxhvVtm1btWvXTs8884z8/f312WeflTl/4cKFGjJkiB599FF17NhRc+bMUY8ePbRo0aJarhwAAADAb7nNNRbFxcVasWKFzpw5o5iYmDLn7NixQ7GxsU5jcXFx2rFjR22UCAAAAKAcpt9udt++fYqJidG5c+fk7++vVatWqVOnTmXOzcrKUmhoqNNYaGiosrKyyt1+YWGhCgsLHe/z8vIkSTabTTabrRo+QdUU2YocP5uxf7gXew/QC/UbfQA7egF29AIk9+iDquzb9GDRvn177d27V7m5uXr//fc1duxYbd68udxwUVWJiYmaNWtWqfENGzbIz8+vWvZRFXnnJfuvPSUlpdb3D/dEL0CiD/AregF29AIkc/ugoKCg0nNNDxbe3t5q06aNJCk6OlpffPGFFi5cqNdee63U3LCwMGVnZzuNZWdnKywsrNztT506VZMnT3a8z8vLU3h4uAYPHqzAwMBq+hSVdyK/UNN2b5YkDRo0SF5eXrVeA9yHzWZTSkoKvVDP0QewoxdgRy9Aco8+sJ/tUxmmB4vfKykpcTp16bdiYmKUmpqqSZMmOcZSUlLKvSZDkqxWq6xWa6lxLy8vU74gT88S02uA+6EXINEH+BW9ADt6AZK5fVCV/ZoaLKZOnaqhQ4eqZcuWOn36tJYvX660tDStX79ekjRmzBg1b95ciYmJkqSHH35Y/fv31/z58zVs2DCtWLFCu3bt0uuvv27mxwAAAADqPVODxfHjxzVmzBhlZmYqKChIUVFRWr9+vQYNGiRJysjIkIfHrzeu6tevn5YvX66nn35aTz75pNq2bavk5GR16dLFrI8AAAAAQCYHi3/+858VLk9LSys1duutt+rWW2+toYoAAAAAXAq3eY4FAAAAgMsXwcIkhixmlwAAAABUG4JFLbOQJwAAAFAHESwAAAAAuIxgAQAAAMBlBAsAAAAALiNYAAAAAHAZwQIAAACAywgWAAAAAFxGsAAAAADgMoJFLeMxFgAAAKiLCBYAAAAAXEawAAAAAOAyggUAAAAAlxEsAAAAALiMYAEAAADAZQQLAAAAAC4jWJjIMAyzSwAAAACqBcECAAAAgMsIFrXMYuEReQAAAKh7CBYAAAAAXEawAAAAAOAyggUAAAAAlxEsAAAAALiMYAEAAADAZQQLE/EYCwAAANQVBAsAAAAALiNY1DKeYgEAAIC6iGABAAAAwGUECwAAAAAuI1gAAAAAcBnBAgAAAIDLCBYAAAAAXEawMBGPsQAAAEBdYWqwSExMVK9evRQQEKCmTZsqISFB6enpFa6TlJQki8Xi9PLx8amligEAAACUxdRgsXnzZk2YMEGfffaZUlJSZLPZNHjwYJ05c6bC9QIDA5WZmel4HT16tJYqdp2FB1kAAACgDvI0c+fr1q1zep+UlKSmTZtq9+7duu6668pdz2KxKCwsrKbLAwAAAFBJpgaL38vNzZUkNWrUqMJ5+fn5ioiIUElJiXr06KFnn31WnTt3LnNuYWGhCgsLHe/z8vIkSTabTTabrZoqrzybreg3P9vUwINDGPWZvQfN6EW4D/oAdvQC7OgFSO7RB1XZt8UwDLe4hrikpEQ33XSTcnJytHXr1nLn7dixQ999952ioqKUm5urF154QVu2bNGBAwfUokWLUvNnzpypWbNmlRpfvny5/Pz8qvUzVMYZm/Tkrgt57sW+RSJXAAAAwF0VFBTo9ttvV25urgIDAyuc6zbB4i9/+YvWrl2rrVu3lhkQymOz2dSxY0eNHj1ac+bMKbW8rCMW4eHhOnHixEV/OTUhp8CmXombJEn7nh4gH6t3rdcA92Gz2ZSSkqJBgwbJy8vL7HJgEvoAdvQC7OgFSO7RB3l5eQoJCalUsHCLU6EmTpyojz/+WFu2bKlSqJAkLy8vde/eXYcPHy5zudVqldVqLXM9M74gLy/jNz+bUwPcD70AiT7Ar+gF2NELkMztg6rs19S7QhmGoYkTJ2rVqlXauHGjIiMjq7yN4uJi7du3T1deeWUNVFiz3ORgEQAAAOAyU49YTJgwQcuXL9eHH36ogIAAZWVlSZKCgoLk6+srSRozZoyaN2+uxMRESdLs2bPVt29ftWnTRjk5OXr++ed19OhR3XPPPaZ9DgAAAKC+MzVYLF68WJI0YMAAp/GlS5dq3LhxkqSMjAx5ePx6YOXUqVO69957lZWVpeDgYEVHR2v79u3q1KlTbZXtEou4WhsAAAB1j6nBojKnAqWlpTm9f/HFF/Xiiy/WUEUAAAAALoWp11gAAAAAqBsIFgAAAABcRrAAAAAA4DKChYm42SwAAADqCoIFAAAAAJcRLAAAAAC4jGBR23iMBQAAAOogggUAAAAAlxEsAAAAALiMYAEAAADAZQQLAAAAAC4jWJjI4EEWAAAAqCMIFgAAAABcRrAAAAAA4DKCBQAAAACXESxqmYUH5AEAAKAOIlgAAAAAcBnBAgAAAIDLCBYAAAAAXEawMBGPsQAAAEBdQbAAAAAA4DKCBQAAAACXESwAAAAAuIxgUct4jAUAAADqIoIFAAAAAJcRLAAAAAC4jGABAAAAwGUECzMZPMkCAAAAdQPBAgAAAIDLCBYAAAAAXEawAAAAAOAygkUts1h4kgUAAADqHoIFAAAAAJcRLAAAAAC4zNRgkZiYqF69eikgIEBNmzZVQkKC0tPTL7reypUr1aFDB/n4+Khr165as2ZNLVQLAAAAoDymBovNmzdrwoQJ+uyzz5SSkiKbzabBgwfrzJkz5a6zfft2jR49Wnfffbf27NmjhIQEJSQkaP/+/bVYefXgKRYAAACoKzzN3Pm6deuc3iclJalp06bavXu3rrvuujLXWbhwoYYMGaJHH31UkjRnzhylpKRo0aJFWrJkSY3XDAAAAKA0U4PF7+Xm5kqSGjVqVO6cHTt2aPLkyU5jcXFxSk5OLnN+YWGhCgsLHe/z8vIkSTabTTabzcWKq85mK/rNzzbZbA1qvQa4D3sPmtGLcB/0AezoBdjRC5Dcow+qsm+3CRYlJSWaNGmSrrnmGnXp0qXceVlZWQoNDXUaCw0NVVZWVpnzExMTNWvWrFLjGzZskJ+fn2tFX4JzxZL9156aulHe5ApISklJMbsEuAH6AHb0AuzoBUjm9kFBQUGl57pNsJgwYYL279+vrVu3Vut2p06d6nSEIy8vT+Hh4Ro8eLACAwOrdV+VcaawSI/v3ChJuuGG6xXg51PrNcB92Gw2paSkaNCgQfLy8jK7HJiEPoAdvQA7egGSe/SB/WyfynCLYDFx4kR9/PHH2rJli1q0aFHh3LCwMGVnZzuNZWdnKywsrMz5VqtVVqu11LiXl5cpX5BXya8PyDOrBrgfegESfYBf0QuwoxcgmdsHVdmvqXeFMgxDEydO1KpVq7Rx40ZFRkZedJ2YmBilpqY6jaWkpCgmJqamygQAAABwEaYesZgwYYKWL1+uDz/8UAEBAY7rJIKCguTr6ytJGjNmjJo3b67ExERJ0sMPP6z+/ftr/vz5GjZsmFasWKFdu3bp9ddfN+1zAAAAAPVdlYJFTk6OVq1apU8//VRHjx5VQUGBmjRpou7duysuLk79+vWr0s4XL14sSRowYIDT+NKlSzVu3DhJUkZGhjw8fj2w0q9fPy1fvlxPP/20nnzySbVt21bJyckVXvDtrgweZAEAAIA6olLB4qefftL06dP19ttvq1mzZurdu7euvvpq+fr66uTJk9q0aZNeeOEFRUREaMaMGRo1alSldm5U4i/rtLS0UmO33nqrbr311krtAwAAAEDNq1Sw6N69u8aOHavdu3erU6dOZc45e/askpOTtWDBAh07dkxTpkyp1kIBAAAAuK9KBYuDBw+qcePGFc7x9fXV6NGjNXr0aP3yyy/VUhwAAACAy0Ol7gp1sVDh6nwAAAAAl7dKX7y9ZcuWSs277rrrLrmY+sBiufgcAAAA4HJT6WAxYMAAWf7vr+LyLrq2WCwqLi6unsoAAAAAXDYqHSyCg4MVEBCgcePG6c9//rNCQkJqsi4AAAAAl5FKP3k7MzNT8+bN044dO9S1a1fdfffd2r59uwIDAxUUFOR4ofIM8SALAAAA1A2VDhbe3t4aNWqU1q9fr2+++UZRUVGaOHGiwsPD9dRTT6moqKgm6wQAAADgxiodLH6rZcuWmj59uj755BO1a9dOc+fOVV5eXnXXBgAAAOAyUeVgUVhYqOXLlys2NlZdunRRSEiIVq9erUaNGtVEfQAAAAAuA5W+eHvnzp1aunSpVqxYoVatWmn8+PF67733CBQAAAAAKh8s+vbtq5YtW+qhhx5SdHS0JGnr1q2l5t10003VV10dZBEPsgAAAEDdU+lgIUkZGRmaM2dOuct5jgUAAABQP1U6WJSUlNRkHQAAAAAuY5d0VyhUj3IeYA4AAABcdiodLL799lvt3LnTaSw1NVUDBw5U79699eyzz1Z7cQAAAAAuD5UOFo8//rg+/vhjx/sjR44oPj5e3t7eiomJUWJiohYsWFATNQIAAABwc5W+xmLXrl167LHHHO/ffvtttWvXTuvXr5ckRUVF6eWXX9akSZOqvUgAAAAA7q3SRyxOnDihFi1aON5v2rRJ8fHxjvcDBgzQ999/X63FAQAAALg8VDpYNGrUSJmZmZIu3CFq165d6tu3r2P5+fPnZXA18kVZeIwFAAAA6qBKB4sBAwZozpw5OnbsmBYsWKCSkhINGDDAsfzgwYNq1apVDZQIAAAAwN1V+hqLZ555RoMGDVJERIQaNGigl156SVdccYVj+bJly3T99dfXSJEAAAAA3Fulg0WrVq106NAhHThwQE2aNFGzZs2cls+aNcvpGgxcHCeOAQAAoK6odLCQJE9PT3Xr1q3MZeWNAwAAAKj7qvTk7TNnzmj69Onq0qWL/P39FRAQoKioKM2ePVsFBQU1VSMAAAAAN1fpIxbnz59X//79tX//fg0dOlTx8fEyDEOHDh3SM888o7Vr12rLli3y8vKqyXoBAAAAuKFKB4vFixfrhx9+0FdffaX27ds7Lfvmm280YMAALVmyRH/961+rvUgAAAAA7q3Sp0J98MEHmjZtWqlQIUkdOnTQU089pffff79aiwMAAABweah0sDh48KDTcyt+b+DAgTp48GB11AQAAADgMlPpYJGTk6PGjRuXu7xx48bKzc2tlqIAAAAAXF4qHSxKSkrUoEGD8jfk4aHi4uJqKaq+MHiQBQAAAOqISl+8bRiGbrjhBnl6lr1KUVFRtRUFAAAA4PJS6WAxY8aMi84ZOXKkS8UAAAAAuDxVa7AAAAAAUD9V6cnb1W3Lli2Kj49Xs2bNZLFYlJycXOH8tLQ0WSyWUq+srKzaKRgAAABAmSoVLIYMGaLPPvvsovNOnz6tefPm6ZVXXqnUzs+cOaNu3bpVer5denq6MjMzHa+mTZtWaX0zWSxmVwAAAABUv0qdCnXrrbdq5MiRCgoKUnx8vHr27KlmzZrJx8dHp06d0sGDB7V161atWbNGw4YN0/PPP1+pnQ8dOlRDhw6tctFNmzZVw4YNq7weAAAAgJpRqWBx9913684779TKlSv17rvv6vXXX3c8s8JisahTp06Ki4vTF198oY4dO9ZowZJ09dVXq7CwUF26dNHMmTN1zTXXlDu3sLBQhYWFjvd5eXmSJJvNJpvNVuO1/p6tqMTxc1GRTTZbpS9zQR1k70EzehHugz6AHb0AO3oBknv0QVX2bTGMS3uaQm5urs6ePavGjRvLy8vrUjbhXIjFolWrVikhIaHcOenp6UpLS1PPnj1VWFioN954Q8uWLdPnn3+uHj16lLnOzJkzNWvWrFLjy5cvl5+fn8t1V1VRifTI5xfCxNxeRfIlVwAAAMBNFRQU6Pbbb1dubq4CAwMrnHvJwaK6VSZYlKV///5q2bKlli1bVubyso5YhIeH68SJExf95dSEwqISdZn1iSTp88euVaMA31qvAe7DZrMpJSVFgwYNqpaAjssTfQA7egF29AIk9+iDvLw8hYSEVCpYXPb/vbx3797aunVrucutVqusVmupcS8vL1O+oBLLr08n9/Ly5B8LSDKvH+Fe6APY0QuwoxcgmdsHVdmvqbebrQ579+7VlVdeaXYZAAAAQL1m6hGL/Px8HT582PH+yJEj2rt3rxo1aqSWLVtq6tSp+vHHH/XWW29JkhYsWKDIyEh17txZ586d0xtvvKGNGzdqw4YNZn0EAAAAADI5WOzatUsDBw50vJ88ebIkaezYsUpKSlJmZqYyMjIcy8+fP69HHnlEP/74o/z8/BQVFaVPPvnEaRsAAAAAal+Vg8WxY8dksVjUokULSdLOnTu1fPlyderUSffdd1+VtjVgwABVdO14UlKS0/vHHntMjz32WFVLdisW8YQ8AAAA1D1Vvsbi9ttv16ZNmyRJWVlZGjRokHbu3KmnnnpKs2fPrvYCAQAAALi/KgeL/fv3q3fv3pKk9957T126dNH27dv19ttvlzrCgIq5x41+AQAAANdVOVjYbDbH7Vs/+eQT3XTTTZKkDh06KDMzs3qrAwAAAHBZqHKw6Ny5s5YsWaJPP/1UKSkpGjJkiCTpp59+UuPGjau9QAAAAADur8rBYt68eXrttdc0YMAAjR49Wt26dZMkffTRR45TpAAAAADUL1W+K9SAAQN04sQJ5eXlKTg42DF+3333yc/Pr1qLAwAAAHB5qPIRi7Nnz6qwsNARKo4ePaoFCxYoPT1dTZs2rfYCAQAAALi/KgeL4cOHO56EnZOToz59+mj+/PlKSEjQ4sWLq73AusbCYywAAABQB1U5WHz55Ze69tprJUnvv/++QkNDdfToUb311lt66aWXqr1AAAAAAO6vysGioKBAAQEBkqQNGzbo5ptvloeHh/r27aujR49We4F1GY+xAAAAQF1R5WDRpk0bJScn69ixY1q/fr0GDx4sSTp+/LgCAwOrvUAAAAAA7q/KwWL69OmaMmWKWrVqpd69eysmJkbShaMX3bt3r/YCAQAAALi/Kt9u9pZbbtEf/vAHZWZmOp5hIUk33HCDRowYUa3FAQAAALg8VDlYSFJYWJjCwsL0ww8/SJJatGjBw/EAAACAeqzKp0KVlJRo9uzZCgoKUkREhCIiItSwYUPNmTNHJSUlNVEjAAAAADdX5SMWTz31lP75z39q7ty5uuaaayRJW7du1cyZM3Xu3Dk988wz1V5kXcJjLAAAAFAXVTlY/Otf/9Ibb7yhm266yTEWFRWl5s2b68EHHyRYAAAAAPVQlU+FOnnypDp06FBqvEOHDjp58mS1FFVfGDzIAgAAAHVElYNFt27dtGjRolLjixYtcrpLFAAAAID6o8qnQj333HMaNmyYPvnkE8czLHbs2KFjx45pzZo11V4gAAAAAPdX5SMW/fv317fffqsRI0YoJydHOTk5uvnmm5Wenq5rr722JmoEAAAA4OYu6TkWzZo1K3WR9g8//KD77rtPr7/+erUUBgAAAODyUeUjFuX55Zdf9M9//rO6NgcAAADgMlJtwQKVY7HwJAsAAADUPQQLAAAAAC4jWJjIEA+yAAAAQN1Q6Yu3b7755gqX5+TkuFoLAAAAgMtUpYNFUFDQRZePGTPG5YIAAAAAXH4qHSyWLl1ak3UAAAAAuIxxjQUAAAAAlxEsAAAAALiMYAEAAADAZQSLWsbj8QAAAFAXESxMZPAYCwAAANQRpgaLLVu2KD4+Xs2aNZPFYlFycvJF10lLS1OPHj1ktVrVpk0bJSUl1XidAAAAACpmarA4c+aMunXrpldeeaVS848cOaJhw4Zp4MCB2rt3ryZNmqR77rlH69evr+FKAQAAAFSk0s+xqAlDhw7V0KFDKz1/yZIlioyM1Pz58yVJHTt21NatW/Xiiy8qLi6upsoEAAAAcBGmBouq2rFjh2JjY53G4uLiNGnSpHLXKSwsVGFhoeN9Xl6eJMlms8lms9VInRUpKfn1woqiInNqgPuwf//0Qf1GH8COXoAdvQDJPfqgKvu+rIJFVlaWQkNDncZCQ0OVl5ens2fPytfXt9Q6iYmJmjVrVqnxDRs2yM/Pr8ZqLc+FXHHh156Wtln+XrVeAtxQSkqK2SXADdAHsKMXYEcvQDK3DwoKCio997IKFpdi6tSpmjx5suN9Xl6ewsPDNXjwYAUGBtZ6PSUlhv722YXmGDCgv5oGXVHrNcB92Gw2paSkaNCgQfLyImXWV/QB7OgF2NELkNyjD+xn+1TGZRUswsLClJ2d7TSWnZ2twMDAMo9WSJLVapXVai017uXlZcoXZPzmHrOenubUAPdjVj/CvdAHsKMXYEcvQDK3D6qy38vqORYxMTFKTU11GktJSVFMTIxJFbmGx1gAAACgrjA1WOTn52vv3r3au3evpAu3k927d68yMjIkXTiNacyYMY75DzzwgP773//qscce0zfffKNXX31V7733nv72t7+ZUT4AAACA/2NqsNi1a5e6d++u7t27S5ImT56s7t27a/r06ZKkzMxMR8iQpMjISK1evVopKSnq1q2b5s+frzfeeINbzQIAAAAmM/UaiwEDBjhdc/B7ZT1Ve8CAAdqzZ08NVgUAAACgqi6raywAAAAAuCeCBQAAAACXESwAAAAAuIxgUcssFovZJQAAAADVjmBhpgouXAcAAAAuJwQLAAAAAC4jWAAAAABwGcECAAAAgMsIFgAAAABcRrAAAAAA4DKCBQAAAACXESwAAAAAuIxgYSKeYgEAAIC6gmABAAAAwGUECwAAAAAuI1gAAAAAcBnBAgAAAIDLCBYAAAAAXEawAAAAAOAygoUJLBazKwAAAACqF8HCRAYPsgAAAEAdQbAAAAAA4DKCBQAAAACXESwAAAAAuIxgAQAAAMBlBAsAAAAALiNYAAAAAHAZwQIAAACAywgWJrA/H4/HWAAAAKCuIFgAAAAAcBnBAgAAAIDLCBYmKOEcKAAAANQxBAsTfZudb3YJAAAAQLVwi2DxyiuvqFWrVvLx8VGfPn20c+fOcucmJSXJYrE4vXx8fGqx2uqz/mC22SUAAAAA1cL0YPHuu+9q8uTJmjFjhr788kt169ZNcXFxOn78eLnrBAYGKjMz0/E6evRoLVYMAAAA4PdMDxb/+Mc/dO+992r8+PHq1KmTlixZIj8/P7355pvlrmOxWBQWFuZ4hYaG1mLFAAAAAH7P1GBx/vx57d69W7GxsY4xDw8PxcbGaseOHeWul5+fr4iICIWHh2v48OE6cOBAbZQLAAAAoByeZu78xIkTKi4uLnXEITQ0VN98802Z67Rv315vvvmmoqKilJubqxdeeEH9+vXTgQMH1KJFi1LzCwsLVVhY6Hifl5cnSbLZbLLZbNX4aaquuLjE9BpgLvv3Tx/Ub/QB7OgF2NELkNyjD6qyb1ODxaWIiYlRTEyM432/fv3UsWNHvfbaa5ozZ06p+YmJiZo1a1ap8Q0bNsjPz69Gay3fhV/7jz/8oDVrMkyqAe4kJSXF7BLgBugD2NELsKMXIJnbBwUFBZWea2qwCAkJUYMGDZSd7Xx3pOzsbIWFhVVqG15eXurevbsOHz5c5vKpU6dq8uTJjvd5eXkKDw/X4MGDFRgYeOnFu+DhHRskSc1btNCNN3YxpQa4B5vNppSUFA0aNEheXl5mlwOT0AewoxdgRy9Aco8+sJ/tUxmmBgtvb29FR0crNTVVCQkJkqSSkhKlpqZq4sSJldpGcXGx9u3bpxtvvLHM5VarVVartdS4l5eX6f9DbdDAw/Qa4B7coR9hPvoAdvQC7OgFSOb2QVX2a/qpUJMnT9bYsWPVs2dP9e7dWwsWLNCZM2c0fvx4SdKYMWPUvHlzJSYmSpJmz56tvn37qk2bNsrJydHzzz+vo0eP6p577jHzY1wSgydwAwAAoI4wPViMGjVKP//8s6ZPn66srCxdffXVWrduneOC7oyMDHl4/HrzqlOnTunee+9VVlaWgoODFR0dre3bt6tTp05mfQQAAACg3jM9WEjSxIkTyz31KS0tzen9iy++qBdffLEWqqp5FovZFQAAAADVw/QH5NVnnAoFAACAuoJgAQAAAMBlBAsAAAAALiNYAAAAAHAZwQIAAACAywgWAAAAAFxGsAAAAADgMoIFAAAAAJcRLAAAAAC4jGABAAAAwGUECwAAAAAuI1gAAAAAcBnBAgAAAIDLCBYmsljMrgAAAACoHgQLExmG2RUAAAAA1YNgAQAAAMBlBAsAAAAALiNYAAAAAHAZwQIAAACAywgWAAAAAFxGsAAAAADgMoIFAAAAAJcRLAAAAAC4jGABAAAAwGUECwAAAAAuI1gAAAAAcBnBwlSG2QUAAAAA1YJgAQAAAMBlBAtTWcwuAAAAAKgWBAsAAAAALiNYAAAAAHAZwQIAAACAywgWJmoa4G12CQAAAEC1IFiYwNPjwkXbLRv5mVwJAAAAUD3cIli88soratWqlXx8fNSnTx/t3LmzwvkrV65Uhw4d5OPjo65du2rNmjW1VGn16B0ZLElK2nHU5EoAAACA6mF6sHj33Xc1efJkzZgxQ19++aW6deumuLg4HT9+vMz527dv1+jRo3X33Xdrz549SkhIUEJCgvbv31/LlV+67f85KUk68NNpkysBAAAAqofpweIf//iH7r33Xo0fP16dOnXSkiVL5OfnpzfffLPM+QsXLtSQIUP06KOPqmPHjpozZ4569OihRYsW1XLlAAAAAOw8zdz5+fPntXv3bk2dOtUx5uHhodjYWO3YsaPMdXbs2KHJkyc7jcXFxSk5ObkmS60x32WflsXxnDwemFffFBUVKfus9J+fz8jT09T/OcJE9AHs6AXY0QuQfu2D0+dsauTlZXY5F2Vqp544cULFxcUKDQ11Gg8NDdU333xT5jpZWVllzs/KyipzfmFhoQoLCx3v8/LyJEk2m002m82V8qvFoBe3mF0CTOepZ/duM7sImI4+gB29ADt6AZLkKf+ITN3as6Upe6/K38t1PgInJiZq1qxZpcY3bNggPz9z7so0urVF7/yngSTpCk9DhqQL/wcAAABw9s3BA1pz3JzriQsKCio919RgERISogYNGig7O9tpPDs7W2FhYWWuExYWVqX5U6dOdTp1Ki8vT+Hh4Ro8eLACAwNd/ASXZpDNpr4pKRo0aJC8LoPDWqg5NptNKfRCvUcfwI5egB29AMk9+sB+tk9lmBosvL29FR0drdTUVCUkJEiSSkpKlJqaqokTJ5a5TkxMjFJTUzVp0iTHWEpKimJiYsqcb7VaZbVaS417eXmZ/j9Ud6gB7oFegEQf4Ff0AuzoBUjm9kFV9mv6qVCTJ0/W2LFj1bNnT/Xu3VsLFizQmTNnNH78eEnSmDFj1Lx5cyUmJkqSHn74YfXv31/z58/XsGHDtGLFCu3atUuvv/66mR8DAAAAqNdMDxajRo3Szz//rOnTpysrK0tXX3211q1b57hAOyMjQx4ev94Vt1+/flq+fLmefvppPfnkk2rbtq2Sk5PVpUsXsz4CAAAAUO+ZHiwkaeLEieWe+pSWllZq7NZbb9Wtt95aw1UBAAAAqCzTH5AHAAAA4PJHsAAAAADgMoIFAAAAAJcRLAAAAAC4jGABAAAAwGUECwAAAAAuI1gAAAAAcJlbPMeiNhmGIUnKy8szrQabzaaCggLl5eWZ9nh2uAd6ARJ9gF/RC7CjFyC5Rx/Y/2a2/w1dkXoXLE6fPi1JCg8PN7kSAAAA4PJw+vRpBQUFVTjHYlQmftQhJSUl+umnnxQQECCLxWJKDXl5eQoPD9exY8cUGBhoSg1wD/QCJPoAv6IXYEcvQHKPPjAMQ6dPn1azZs3k4VHxVRT17oiFh4eHWrRoYXYZkqTAwED+sYAkegEX0AewoxdgRy9AMr8PLnakwo6LtwEAAAC4jGABAAAAwGUECxNYrVbNmDFDVqvV7FJgMnoBEn2AX9ELsKMXIF1+fVDvLt4GAAAAUP04YgEAAADAZQQLAAAAAC4jWAAAAABwGcGilr3yyitq1aqVfHx81KdPH+3cudPskuCiLVu2KD4+Xs2aNZPFYlFycrLTcsMwNH36dF155ZXy9fVVbGysvvvuO6c5J0+e1B133KHAwEA1bNhQd999t/Lz853mfP3117r22mvl4+Oj8PBwPffcczX90VAFiYmJ6tWrlwICAtS0aVMlJCQoPT3dac65c+c0YcIENW7cWP7+/ho5cqSys7Od5mRkZGjYsGHy8/NT06ZN9eijj6qoqMhpTlpamnr06CGr1ao2bdooKSmppj8eqmDx4sWKiopy3Hc+JiZGa9eudSynD+qnuXPnymKxaNKkSY4xeqF+mDlzpiwWi9OrQ4cOjuV1qg8M1JoVK1YY3t7exptvvmkcOHDAuPfee42GDRsa2dnZZpcGF6xZs8Z46qmnjA8++MCQZKxatcpp+dy5c42goCAjOTnZ+Oqrr4ybbrrJiIyMNM6ePeuYM2TIEKNbt27GZ599Znz66adGmzZtjNGjRzuW5+bmGqGhocYdd9xh7N+/33jnnXcMX19f47XXXqutj4mLiIuLM5YuXWrs37/f2Lt3r3HjjTcaLVu2NPLz8x1zHnjgASM8PNxITU01du3aZfTt29fo16+fY3lRUZHRpUsXIzY21tizZ4+xZs0aIyQkxJg6dapjzn//+1/Dz8/PmDx5snHw4EHj5ZdfNho0aGCsW7euVj8vyvfRRx8Zq1evNr799lsjPT3dePLJJw0vLy9j//79hmHQB/XRzp07jVatWhlRUVHGww8/7BinF+qHGTNmGJ07dzYyMzMdr59//tmxvC71AcGiFvXu3duYMGGC431xcbHRrFkzIzEx0cSqUJ1+HyxKSkqMsLAw4/nnn3eM5eTkGFar1XjnnXcMwzCMgwcPGpKML774wjFn7dq1hsViMX788UfDMAzj1VdfNYKDg43CwkLHnMcff9xo3759DX8iXKrjx48bkozNmzcbhnHhe/fy8jJWrlzpmHPo0CFDkrFjxw7DMC6EVA8PDyMrK8sxZ/HixUZgYKDju3/ssceMzp07O+1r1KhRRlxcXE1/JLggODjYeOONN+iDeuj06dNG27ZtjZSUFKN///6OYEEv1B8zZswwunXrVuayutYHnApVS86fP6/du3crNjbWMebh4aHY2Fjt2LHDxMpQk44cOaKsrCyn7z0oKEh9+vRxfO87duxQw4YN1bNnT8ec2NhYeXh46PPPP3fMue666+Tt7e2YExcXp/T0dJ06daqWPg2qIjc3V5LUqFEjSdLu3btls9mceqFDhw5q2bKlUy907dpVoaGhjjlxcXHKy8vTgQMHHHN+uw37HP4dcU/FxcVasWKFzpw5o5iYGPqgHpowYYKGDRtW6vuiF+qX7777Ts2aNdNVV12lO+64QxkZGZLqXh8QLGrJiRMnVFxc7NQUkhQaGqqsrCyTqkJNs3+3FX3vWVlZatq0qdNyT09PNWrUyGlOWdv47T7gPkpKSjRp0iRdc8016tKli6QL35O3t7caNmzoNPf3vXCx77m8OXl5eTp79mxNfBxcgn379snf319Wq1UPPPCAVq1apU6dOtEH9cyKFSv05ZdfKjExsdQyeqH+6NOnj5KSkrRu3TotXrxYR44c0bXXXqvTp0/XuT7wrLU9AUA9MWHCBO3fv19bt241uxSYpH379tq7d69yc3P1/vvva+zYsdq8ebPZZaEWHTt2TA8//LBSUlLk4+Njdjkw0dChQx0/R0VFqU+fPoqIiNB7770nX19fEyurfhyxqCUhISFq0KBBqav8s7OzFRYWZlJVqGn277ai7z0sLEzHjx93Wl5UVKSTJ086zSlrG7/dB9zDxIkT9fHHH2vTpk1q0aKFYzwsLEznz59XTk6O0/zf98LFvufy5gQGBta5/wd1OfP29labNm0UHR2txMREdevWTQsXLqQP6pHdu3fr+PHj6tGjhzw9PeXp6anNmzfrpZdekqenp0JDQ+mFeqphw4Zq166dDh8+XOf+TSBY1BJvb29FR0crNTXVMVZSUqLU1FTFxMSYWBlqUmRkpMLCwpy+97y8PH3++eeO7z0mJkY5OTnavXu3Y87GjRtVUlKiPn36OOZs2bJFNpvNMSclJUXt27dXcHBwLX0aVMQwDE2cOFGrVq3Sxo0bFRkZ6bQ8OjpaXl5eTr2Qnp6ujIwMp17Yt2+fU9BMSUlRYGCgOnXq5Jjz223Y5/DviHsrKSlRYWEhfVCP3HDDDdq3b5/27t3rePXs2VN33HGH42d6oX7Kz8/Xf/7zH1155ZV179+EWr1UvJ5bsWKFYbVajaSkJOPgwYPGfffdZzRs2NDpKn9cfk6fPm3s2bPH2LNnjyHJ+Mc//mHs2bPHOHr0qGEYF24327BhQ+PDDz80vv76a2P48OFl3m62e/fuxueff25s3brVaNu2rdPtZnNycozQ0FDjz3/+s7F//35jxYoVhp+fH7ebdSN/+ctfjKCgICMtLc3ploIFBQWOOQ888IDRsmVLY+PGjcauXbuMmJgYIyYmxrHcfkvBwYMHG3v37jXWrVtnNGnSpMxbCj766KPGoUOHjFdeeYVbS7qZJ554wti8ebNx5MgR4+uvvzaeeOIJw2KxGBs2bDAMgz6oz357VyjDoBfqi0ceecRIS0szjhw5Ymzbts2IjY01QkJCjOPHjxuGUbf6gGBRy15++WWjZcuWhre3t9G7d2/js88+M7skuGjTpk2GpFKvsWPHGoZx4Zaz06ZNM0JDQw2r1WrccMMNRnp6utM2fvnlF2P06NGGv7+/ERgYaIwfP944ffq005yvvvrK+MMf/mBYrVajefPmxty5c2vrI6ISyuoBScbSpUsdc86ePWs8+OCDRnBwsOHn52eMGDHCyMzMdNrO999/bwwdOtTw9fU1QkJCjEceecSw2WxOczZt2mRcffXVhre3t3HVVVc57QPmu+uuu4yIiAjD29vbaNKkiXHDDTc4QoVh0Af12e+DBb1QP4waNcq48sorDW9vb6N58+bGqFGjjMOHDzuW16U+sBiGYdTuMRIAAAAAdQ3XWAAAAABwGcECAAAAgMsIFgAAAABcRrAAAAAA4DKCBQAAAACXESwAAAAAuIxgAQAAAMBlBAsAAAAALiNYAACqpFWrVlqwYEGl56elpclisSgnJ6fGagIAmI9gAQB1lMViqfA1c+bMS9ruF198ofvuu6/S8/v166fMzEwFBQVd0v6qA+EGAGqep9kFAABqRmZmpuPnd999V9OnT1d6erpjzN/f3/GzYRgqLi6Wp+fF/99CkyZNqlSHt7e3wsLCqrQOAODywxELAKijwsLCHK+goCBZLBbH+2+++UYBAQFau3atoqOjZbVatXXrVv3nP//R8OHDFRoaKn9/f/Xq1UuffPKJ03Z/fyqUxWLRG2+8oREjRsjPz09t27bVRx995Fj++6MFSUlJatiwodavX6+OHTvK399fQ4YMcQpCRUVFeuihh9SwYUM1btxYjz/+uMaOHauEhIRyP+/Ro0cVHx+v4OBgXXHFFercubPWrFmj77//XgMHDpQkBQcHy2KxaNy4cZKkkpISJSYmKjIyUr6+vurWrZvef//9UrWvXr1aUVFR8vHxUd++fbV///5L/FYAoO4iWABAPfbEE09o7ty5OnTokKKiopSfn68bb7xRqamp2rNnj4YMGaL4+HhlZGRUuJ1Zs2bptttu09dff60bb7xRd9xxh06ePFnu/IKCAr3wwgtatmyZtmzZooyMDE2ZMsWxfN68eXr77be1dOlSbdu2TXl5eUpOTq6whgkTJqiwsFBbtmzRvn37NG/ePPn7+ys8PFz/7//9P0lSenq6MjMztXDhQklSYmKi3nrrLS1ZskQHDhzQ3/72N915553avHmz07YfffRRzZ8/X1988YWaNGmi+Ph42Wy2CusBgHrHAADUeUuXLjWCgoIc7zdt2mRIMpKTky+6bufOnY2XX37Z8T4iIsJ48cUXHe8lGU8//bTjfX5+viHJWLt2rdO+Tp065ahFknH48GHHOq+88ooRGhrqeB8aGmo8//zzjvdFRUVGy5YtjeHDh5dbZ9euXY2ZM2eWuez3NRiGYZw7d87w8/Mztm/f7jT37rvvNkaPHu203ooVKxzLf/nlF8PX19d49913y60FAOojrrEAgHqsZ8+eTu/z8/M1c+ZMrV69WpmZmSoqKtLZs2cvesQiKirK8fMVV1yhwMBAHT9+vNz5fn5+at26teP9lVde6Zifm5ur7Oxs9e7d27G8QYMGio6OVklJSbnbfOihh/SXv/xFGzZsUGxsrEaOHOlU1+8dPnxYBQUFGjRokNP4+fPn1b17d6exmJgYx8+NGjVS+/btdejQoXK3DQD1EcECAOqxK664wun9lClTlJKSohdeeEFt2rSRr6+vbrnlFp0/f77C7Xh5eTm9t1gsFYaAsuYbhlHF6p3dc889iouL0+rVq7VhwwYlJiZq/vz5+utf/1rm/Pz8fEnS6tWr1bx5c6dlVqvVpVoAoD7iGgsAgMO2bds0btw4jRgxQl27dlVYWJi+//77Wq0hKChIoaGh+uKLLxxjxcXF+vLLLy+6bnh4uB544AF98MEHeuSRR/Q///M/ki7cmcq+HbtOnTrJarUqIyNDbdq0cXqFh4c7bfezzz5z/Hzq1Cl9++236tixo0ufEwDqGo5YAAAc2rZtqw8++EDx8fGyWCyaNm1ahUceaspf//pXJSYmqk2bNurQoYNefvllnTp1ShaLpdx1Jk2apKFDh6pdu3Y6deqUNm3a5PjjPyIiQhaLRR9//LFuvPFG+fr6KiAgQFOmTNHf/vY3lZSU6A9/+INyc3O1bds2BQYGauzYsY5tz549W40bN1ZoaKieeuophYSEVHiHKgCojzhiAQBw+Mc//qHg4GD169dP8fHxiouLU48ePWq9jscff1yjR4/WmDFjFBMTI39/f8XFxcnHx6fcdYqLizVhwgR17NhRQ4YMUbt27fTqq69Kkpo3b65Zs2bpiSeeUGhoqCZOnChJmjNnjqZNm6bExETHeqtXr1ZkZKTTtufOnauHH35Y0dHRysrK0r///W/HURAAwAUWw9WTWgEAqGElJSXq2LGjbrvtNs2ZM6fW9puWlqaBAwfq1KlTatiwYa3tFwAuR5wKBQBwO0ePHtWGDRvUv39/FRYWatGiRTpy5Ihuv/12s0sDAJSDU6EAAG7Hw8NDSUlJ6tWrl6655hrt27dPn3zyCRdMA4Ab41QoAAAAAC7jiAUAAAAAlxEsAAAAALiMYAEAAADAZQQLAAAAAC4jWAAAAABwGcECAAAAgMsIFgAAAABcRrAAAAAA4DKCBQAAAACX/X+Oxzm9RT+uzgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve(loss_list):\n",
    "    \"\"\"\n",
    "    loss_list: Python list or Tensor containing loss values for each step\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(loss_list, linewidth=1.5)\n",
    "    plt.xlabel(\"Training step\")\n",
    "    plt.ylabel(\"Loss (DSM)\")\n",
    "    plt.title(\"DSM Training Loss Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss_curve(loss_history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0259c7f",
   "metadata": {},
   "source": [
    "## 試著做做看 validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30498945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] step 5000 | val_loss=2.4761\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss_x0(model, batch_size):\n",
    "    x0 = sample_x0_cloud(batch_size)\n",
    "    t = torch.randint(int(0.1*T), T, (batch_size,), device=device)\n",
    "\n",
    "    alpha_bar_t = alpha_bar[t].view(-1,1,1)\n",
    "    sigma_t     = sigmas[t].view(-1,1,1)\n",
    "\n",
    "    eps = torch.randn_like(x0)\n",
    "    x_t = torch.sqrt(alpha_bar_t)*x0 + sigma_t*eps\n",
    "\n",
    "    x0_hat = model(x_t, t)\n",
    "\n",
    "    return ((x0_hat - x0)**2).mean().item()\n",
    "\n",
    "if step % 500 == 0:\n",
    "    vloss = val_loss_x0(model, batch_size)\n",
    "    print(f\"[VAL] step {step} | val_loss={vloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d8a36",
   "metadata": {},
   "source": [
    "## Reverse Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7397732",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_sample_eps(model, num_samples=200, n_points=N_POINTS):\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.randn(num_samples, n_points, 3, device=device)\n",
    "\n",
    "    for t_idx in reversed(range(1, T)):\n",
    "        t = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n",
    "\n",
    "        eps_hat = model(x, t)  # (B,N,3)\n",
    "\n",
    "        beta_t      = betas[t_idx]\n",
    "        alpha_t     = alphas[t_idx]\n",
    "        alpha_bar_t = alpha_bar[t_idx]\n",
    "\n",
    "        coef1 = 1 / torch.sqrt(alpha_t)\n",
    "        coef2 = beta_t / torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # DDPM mean for point cloud\n",
    "        x0_pred = (x - coef2 * eps_hat) / torch.sqrt(alpha_t)\n",
    "        mean = coef1 * (x - coef2 * eps_hat)\n",
    "\n",
    "        # DDPM variance term\n",
    "        beta_tilde = beta_t * (1 - alpha_bar[t_idx-1]) / (1 - alpha_bar_t)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        x = mean + torch.sqrt(beta_tilde) * noise\n",
    "\n",
    "    return x.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292f06",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efd990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scatter_2d_pc(samples):\n",
    "    # samples: (B, N, 3)\n",
    "    pc = samples.reshape(-1, 3)   # (B*N, 3)\n",
    "\n",
    "    x = pc[:, 0]\n",
    "    y = pc[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(x, y, s=8, alpha=0.5)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Reverse Diffusion: Point Cloud Scatter (all points)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "607fef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_x_pc(samples):\n",
    "    pc = samples.reshape(-1, 3)   # (B*N, 3)\n",
    "    x = pc[:, 0]\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=40, density=True, alpha=0.7)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.title(\"Histogram of x (point cloud mixture)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b19a6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def plot_kde_heatmap_pc(samples):\n",
    "    pc = samples.reshape(-1, 3)\n",
    "    x = pc[:, 0]\n",
    "    y = pc[:, 1]\n",
    "\n",
    "    xx, yy = np.mgrid[min(x)-1:max(x)+1:200j,\n",
    "                      min(y)-1:max(y)+1:200j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.contourf(xx, yy, f, levels=50, cmap=\"viridis\")\n",
    "    plt.title(\"2D KDE (point cloud)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1818a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plot_scatter_3d_pc(samples):\n",
    "    pc = samples.reshape(-1, 3)\n",
    "    x = pc[:, 0]\n",
    "    y = pc[:, 1]\n",
    "    z = pc[:, 2]\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(x, y, z, s=5, alpha=0.5)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_zlabel(\"z\")\n",
    "    plt.title(\"3D Scatter of Point Cloud Samples\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "885f739a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_sample_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, n_points=1)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(samples\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[54], line 10\u001b[0m, in \u001b[0;36mreverse_sample_eps\u001b[1;34m(model, num_samples, n_points)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T)):\n\u001b[0;32m      8\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((num_samples,), t_idx, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 10\u001b[0m     eps_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B,N,3)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     beta_t      \u001b[38;5;241m=\u001b[39m betas[t_idx]\n\u001b[0;32m     13\u001b[0m     alpha_t     \u001b[38;5;241m=\u001b[39m alphas[t_idx]\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[50], line 105\u001b[0m, in \u001b[0;36mEGNN_Eps.forward\u001b[1;34m(self, x_t, t)\u001b[0m\n\u001b[0;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m x_t\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 105\u001b[0m     x, h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# 輸出 eps_hat\u001b[39;00m\n\u001b[0;32m    108\u001b[0m eps_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_mlp(torch\u001b[38;5;241m.\u001b[39mcat([x, h], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[50], line 52\u001b[0m, in \u001b[0;36mEGNNLayer.forward\u001b[1;34m(self, x, h, t_emb)\u001b[0m\n\u001b[0;32m     49\u001b[0m t_in \u001b[38;5;241m=\u001b[39m t_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, N, N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m edge_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h_i, h_j, r2, t_in], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m e_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# coordinate update\u001b[39;00m\n\u001b[0;32m     55\u001b[0m w_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_mlp(e_ij)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = reverse_sample_eps(model, num_samples=200)#, n_points=1)\n",
    "print(samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111917ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_2d_pc(samples)\n",
    "plot_histogram_x_pc(samples)\n",
    "plot_kde_heatmap_pc(samples)\n",
    "plot_scatter_3d_pc(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
