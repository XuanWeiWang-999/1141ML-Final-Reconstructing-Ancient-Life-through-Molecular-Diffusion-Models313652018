{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae916ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===========================\n",
    "# Diffusion schedule\n",
    "# ===========================\n",
    "T = 1000\n",
    "beta_start = 1e-4\n",
    "beta_end   = 2e-2\n",
    "betas = torch.linspace(beta_start, beta_end, T, device=device)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "sigmas = torch.sqrt(1.0 - alpha_bar)\n",
    "\n",
    "# ===========================\n",
    "# Data: mixture Gaussian -> point cloud\n",
    "# ===========================\n",
    "mu1 = torch.tensor([+2.0, 0.0, 0.0], device=device)\n",
    "mu2 = torch.tensor([-2.0, 0.0, 0.0], device=device)\n",
    "\n",
    "N_POINTS = 64\n",
    "\n",
    "def sample_x0_cloud(batch_size: int, n_points: int = N_POINTS):\n",
    "    B = batch_size\n",
    "    mix = torch.bernoulli(0.5*torch.ones(B, n_points, device=device))  # (B,N)\n",
    "    mu = torch.where(mix.unsqueeze(-1)==1, mu1, mu2)  # (B,N,3)\n",
    "    eps = torch.randn(B, n_points, 3, device=device)\n",
    "    return mu + eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0be11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Fourier time embedding\n",
    "# ===========================\n",
    "class FourierTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim=64, max_freq=1000.0):\n",
    "        super().__init__()\n",
    "        self.freqs = torch.exp(\n",
    "            torch.linspace(0, math.log(max_freq), dim//2)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_norm = t.float() / float(T)\n",
    "        args = t_norm.unsqueeze(-1) * self.freqs.to(t.device)\n",
    "        return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "\n",
    "# ===========================\n",
    "# EGNN Layer\n",
    "# ===========================\n",
    "class EGNNLayer(nn.Module):\n",
    "    def __init__(self, time_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(1 + time_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Tanh()           # keep weights stable\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        B, N, _ = x.shape\n",
    "\n",
    "        diff = x.unsqueeze(2) - x.unsqueeze(1)           # (B,N,N,3)\n",
    "        r2 = (diff**2).sum(dim=-1, keepdim=True)         # (B,N,N,1)\n",
    "\n",
    "        te = t_emb.unsqueeze(1).unsqueeze(2).expand(B,N,N,-1)\n",
    "        h = torch.cat([r2, te], dim=-1)\n",
    "\n",
    "        w = self.edge_mlp(h)                             # (B,N,N,1)\n",
    "\n",
    "        dx = (w * diff).sum(dim=2)                       # (B,N,3)\n",
    "\n",
    "        return x + dx\n",
    "\n",
    "# ===========================\n",
    "# EGNN model: predicts x0 (not score)\n",
    "# ===========================\n",
    "class EGNN_x0(nn.Module):\n",
    "    def __init__(self, n_layers=3, time_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.t_emb = FourierTimeEmbedding(time_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EGNNLayer(time_dim=time_dim, hidden_dim=hidden_dim)\n",
    "             for _ in range(n_layers)]\n",
    "        )\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        B, N, _ = x_t.shape\n",
    "        t_emb = self.t_emb(t)\n",
    "\n",
    "        x = x_t\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, t_emb)\n",
    "\n",
    "        x_flat = x.reshape(B*N, 3)\n",
    "        x0_flat = self.out_mlp(x_flat)\n",
    "        return x0_flat.reshape(B, N, 3)\n",
    "\n",
    "model = EGNN_x0().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eeae8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   200 | loss=82060.0859\n",
      "step   400 | loss=13802.3799\n",
      "step   600 | loss=2576.5325\n",
      "step   800 | loss=4201.8862\n",
      "step  1000 | loss=543.0429\n",
      "step  1200 | loss=1893.8910\n",
      "step  1400 | loss=2017.7432\n",
      "step  1600 | loss=1149.2738\n",
      "step  1800 | loss=19.5828\n",
      "step  2000 | loss=17.8711\n",
      "step  2200 | loss=98.6815\n",
      "step  2400 | loss=10.8795\n",
      "step  2600 | loss=188.3660\n",
      "step  2800 | loss=1393.7628\n",
      "step  3000 | loss=3816.8313\n",
      "step  3200 | loss=617.4590\n",
      "step  3400 | loss=6.9975\n",
      "step  3600 | loss=9576.6611\n",
      "step  3800 | loss=1398.7972\n",
      "step  4000 | loss=12.6269\n",
      "step  4200 | loss=1784.3511\n",
      "step  4400 | loss=251.9515\n",
      "step  4600 | loss=1251.1548\n",
      "step  4800 | loss=3.9039\n",
      "step  5000 | loss=3676.2754\n",
      "step  5200 | loss=5.2457\n",
      "step  5400 | loss=43.9872\n",
      "step  5600 | loss=2.9570\n",
      "step  5800 | loss=3.1693\n",
      "step  6000 | loss=11.9074\n",
      "step  6200 | loss=18.7120\n",
      "step  6400 | loss=26.8876\n",
      "step  6600 | loss=4.4952\n",
      "step  6800 | loss=3.5921\n",
      "step  7000 | loss=18.0147\n",
      "step  7200 | loss=2.7119\n",
      "step  7400 | loss=421.3731\n",
      "step  7600 | loss=276.8195\n",
      "step  7800 | loss=6.9663\n",
      "step  8000 | loss=11.6500\n",
      "step  8200 | loss=7.7965\n",
      "step  8400 | loss=24.0848\n",
      "step  8600 | loss=119.4509\n",
      "step  8800 | loss=4.1772\n",
      "step  9000 | loss=7.6464\n",
      "step  9200 | loss=2.5399\n",
      "step  9400 | loss=10.2215\n",
      "step  9600 | loss=5.5353\n",
      "step  9800 | loss=2.7864\n",
      "step 10000 | loss=15.0365\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Loss: predict x0\n",
    "# ===========================\n",
    "def loss_x0(model, batch_size):\n",
    "    x0 = sample_x0_cloud(batch_size)\n",
    "\n",
    "    t = torch.randint(0, T, (batch_size,), device=device)\n",
    "    alpha_bar_t = alpha_bar[t].view(-1,1,1)\n",
    "    sigma_t     = sigmas[t].view(-1,1,1)\n",
    "\n",
    "    eps = torch.randn_like(x0)\n",
    "    x_t = torch.sqrt(alpha_bar_t)*x0 + sigma_t*eps\n",
    "\n",
    "    x0_hat = model(x_t, t)\n",
    "\n",
    "    return ((x0_hat - x0)**2).mean()\n",
    "\n",
    "# ===========================\n",
    "# Training loop\n",
    "# ===========================\n",
    "batch_size = 8\n",
    "lr = 2e-4\n",
    "num_steps = 10000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=200,\n",
    "    cooldown=100,\n",
    "    min_lr=1e-8\n",
    ")\n",
    "loss_history = []\n",
    "\n",
    "model.train()\n",
    "for step in range(1, num_steps+1):\n",
    "    loss = loss_x0(model, batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"step {step:5d} | loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f129e9b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2337073829.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss_history.pop([1:10])\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve(loss_list):\n",
    "    \"\"\"\n",
    "    loss_list: Python list or Tensor containing loss values for each step\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(loss_list, linewidth=1.5)\n",
    "    plt.xlabel(\"Training step\")\n",
    "    plt.ylabel(\"Loss (DSM)\")\n",
    "    plt.title(\"DSM Training Loss Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "loss_history.pop([1:10])\n",
    "plot_loss_curve(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0259c7f",
   "metadata": {},
   "source": [
    "## 試著做做看 validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30498945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] step 10000 | val_loss=39.5570\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def val_loss_x0(model, batch_size):\n",
    "    x0 = sample_x0_cloud(batch_size)\n",
    "    t = torch.randint(int(0.1*T), T, (batch_size,), device=device)\n",
    "\n",
    "    alpha_bar_t = alpha_bar[t].view(-1,1,1)\n",
    "    sigma_t     = sigmas[t].view(-1,1,1)\n",
    "\n",
    "    eps = torch.randn_like(x0)\n",
    "    x_t = torch.sqrt(alpha_bar_t)*x0 + sigma_t*eps\n",
    "\n",
    "    x0_hat = model(x_t, t)\n",
    "\n",
    "    return ((x0_hat - x0)**2).mean().item()\n",
    "\n",
    "if step % 500 == 0:\n",
    "    vloss = val_loss_x0(model, batch_size)\n",
    "    print(f\"[VAL] step {step} | val_loss={vloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d8a36",
   "metadata": {},
   "source": [
    "## Reverse Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7397732",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_sample(model, num_samples=200, n_points=N_POINTS):\n",
    "    \"\"\"\n",
    "    反向 diffusion → 生成點雲\n",
    "    產生 shape = (num_samples, n_points, 3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 先從純 Gaussian 噪聲開始\n",
    "    x_t = torch.randn(num_samples, n_points, 3, device=device)\n",
    "\n",
    "    for t in range(T-1, -1, -1):\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # predict x0\n",
    "        x0_hat = model(x_t, t_batch)   # (B,N,3)\n",
    "\n",
    "        alpha_bar_t   = alpha_bar[t]\n",
    "        if t > 0:\n",
    "            alpha_bar_prev = alpha_bar[t-1]\n",
    "        else:\n",
    "            alpha_bar_prev = torch.tensor(1.0, device=device)\n",
    "\n",
    "        # reconstruction step\n",
    "        mean = torch.sqrt(alpha_bar_prev) * x0_hat\n",
    "        var  = 1 - alpha_bar_prev\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            x_t = mean + torch.sqrt(var) * noise\n",
    "        else:\n",
    "            x_t = mean\n",
    "\n",
    "    return x_t.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292f06",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd990bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scatter_2d(samples):\n",
    "    # samples shape = (B, N, 3)\n",
    "    pts = samples.reshape(-1, 3)\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(x, y, s=5, alpha=0.5)\n",
    "    plt.title(\"Reverse Diffusion: 2D Scatter (x-y plane)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607fef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_x(samples):\n",
    "    pts = samples.reshape(-1, 3)\n",
    "    x = pts[:,0]\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=50, density=True, alpha=0.7)\n",
    "    plt.title(\"1D Histogram of x (should show bimodal mixture)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"density\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b19a6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_kde_heatmap(samples):\n",
    "    pts = samples.reshape(-1, 3)\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.kdeplot(x=x, y=y, fill=True, cmap=\"viridis\", thresh=0.05, levels=30)\n",
    "    plt.title(\"2D KDE Heatmap (Density of Generated Samples)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1818a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_scatter_3d(samples):\n",
    "    pts = samples.reshape(-1, 3)\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "    z = pts[:,2]\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, s=5, alpha=0.4)\n",
    "\n",
    "    ax.set_title(\"3D Scatter of Generated Samples\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_zlabel(\"z\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "885f739a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plot_scatter_2d(samples)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#plot_histogram_x(samples)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#plot_kde_heatmap(samples)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#plot_scatter_3d(samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mreverse_sample\u001b[1;34m(model, num_samples, n_points)\u001b[0m\n\u001b[0;32m     13\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((num_samples,), t, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# predict x0\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m x0_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B,N,3)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m alpha_bar_t   \u001b[38;5;241m=\u001b[39m alpha_bar[t]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36mEGNN_x0.forward\u001b[1;34m(self, x_t, t)\u001b[0m\n\u001b[0;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m x_t\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mN, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     70\u001b[0m x0_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_mlp(x_flat)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mEGNNLayer.forward\u001b[1;34m(self, x, t_emb)\u001b[0m\n\u001b[0;32m     35\u001b[0m te \u001b[38;5;241m=\u001b[39m t_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B,N,N,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([r2, te], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m                             \u001b[38;5;66;03m# (B,N,N,1)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m dx \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m*\u001b[39m diff)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)                       \u001b[38;5;66;03m# (B,N,3)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m dx\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\ml_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = reverse_sample(model, num_samples=200, n_points=32)\n",
    "\n",
    "plot_scatter_2d(samples)\n",
    "#plot_histogram_x(samples)\n",
    "#plot_kde_heatmap(samples)\n",
    "#plot_scatter_3d(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b35055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
